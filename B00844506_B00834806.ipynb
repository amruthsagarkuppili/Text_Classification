{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACTION OF DATA FROM XML FILE\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import numpy\n",
    "import numpy as np\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, davies_bouldin_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn import preprocessing\n",
    "from keras import Sequential\n",
    "\n",
    "\n",
    "dataColumns = [\"headline\", \"text\", \"bip:topics\", \"dc.date.published\", \"itemID\", \"XML_File_Name\"]\n",
    "clusterDataframeList = []\n",
    "rows = []\n",
    "paragraph = \"\"\n",
    "bipTopicList = []\n",
    "vec = CountVectorizer(stop_words=None)\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=10000, max_df=0.5, use_idf=True)\n",
    "enhancedDFList = []\n",
    "\n",
    "\n",
    "def dataExtraction():\n",
    "    dir = '/Users/amruthkuppili/Downloads/Sample/'\n",
    "    for file in glob.iglob(os.path.join(dir, '*/*.xml')):\n",
    "        paragraph = \"\"\n",
    "        bipTopicCode = \"\"\n",
    "        path, fileName = os.path.split(file)  # Obtained File name\n",
    "        data = etree.parse(file)\n",
    "        root = data.getroot()\n",
    "        itemId = data.getroot().attrib.get(\"itemid\")  # Obtained item ID\n",
    "        headline = data.find(\"headline\").text\n",
    "        textNode = data.find(\"text\")\n",
    "        for node in textNode:\n",
    "            paragraph = paragraph + node.text  # Obtained text\n",
    "        dcPublishedNode = root.findall(\"./metadata/dc[@element='dc.date.published']\")\n",
    "        if dcPublishedNode is not None:\n",
    "            published_date = dcPublishedNode[0].attrib.get(\"value\")  # obtained dc.date.published\n",
    "        else:\n",
    "            published_date = \"NONE\"\n",
    "        bipNode = root.findall(\"./metadata/codes[@class='bip:topics:1.0']/code\")\n",
    "        text = removeStopWords(paragraph)  # removing stop words\n",
    "        if bipNode is not None:\n",
    "            for innercodes in bipNode:\n",
    "                bipTopicCode = innercodes.attrib.get(\"code\")  # obtained bip:topic code\n",
    "                rows.append({\"itemID\": itemId, \"XML_File_Name\": fileName, \"headline\": headline, \"text\": text,\n",
    "                             \"dc.date.published\": published_date, \"bip:topics\": bipTopicCode})\n",
    "                uniqueBipTopics(bipTopicCode)\n",
    "                break\n",
    "        else:\n",
    "            bipTopicCode = \"NONE\"\n",
    "            rows.append({\"itemID\": itemId, \"XML_File_Name\": fileName, \"headline\": headline, \"text\": text,\n",
    "                         \"dc.date.published\": published_date, \"bip:topics\": bipTopicCode})\n",
    "\n",
    "    customDataFrame = pd.DataFrame(rows, columns=dataColumns)\n",
    "    return customDataFrame\n",
    "\n",
    "\n",
    "def uniqueBipTopics(topic):\n",
    "    if topic not in bipTopicList:\n",
    "        bipTopicList.append(topic)\n",
    "    return bipTopicList\n",
    "\n",
    "\n",
    "def removeStopWords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_tokens = word_tokenize(text)\n",
    "    filtered_sentence_list = [w for w in text_tokens if w not in stop_words]\n",
    "    filtered_lemmatized_list = lemmatization(filtered_sentence_list)\n",
    "    filtered_stemmed_list = stemming(filtered_lemmatized_list)\n",
    "    filtered_lemmatized_sentence = ' '.join(filtered_stemmed_list)\n",
    "    return filtered_lemmatized_sentence\n",
    "\n",
    "\n",
    "def stemming(sentence):\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_words = []\n",
    "    for w in sentence:\n",
    "        stemmed_words.append(ps.stem(w))\n",
    "    return stemmed_words\n",
    "\n",
    "\n",
    "def lemmatization(filtered_sentence):\n",
    "    lem = WordNetLemmatizer()\n",
    "    lemmatized_words = []\n",
    "    for w in filtered_sentence:\n",
    "        lemmatized_words.append(lem.lemmatize(w))\n",
    "    return lemmatized_words\n",
    "rawDataFrame = dataExtraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "      <th>bip:topics</th>\n",
       "      <th>dc.date.published</th>\n",
       "      <th>itemID</th>\n",
       "      <th>XML_File_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Care Group Inc Q4 shr loss vs profit.</td>\n",
       "      <td>( 000 's omit ) year end decemb 31 ( audit ) 1...</td>\n",
       "      <td>C15</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>476242</td>\n",
       "      <td>476242newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>China economy to grow at more than 10 pct in 1...</td>\n",
       "      <td>china see econom growth 10 percent year , infl...</td>\n",
       "      <td>E11</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>476768</td>\n",
       "      <td>476768newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Africans say global debt relief plan too rigid.</td>\n",
       "      <td>african financi leader criticis monday new int...</td>\n",
       "      <td>E51</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>476074</td>\n",
       "      <td>476074newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Argentine export grain prices - March 31.</td>\n",
       "      <td>export offer/bid price u.s. dollar per tonn , ...</td>\n",
       "      <td>M14</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>476597</td>\n",
       "      <td>476597newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Yemen says kidnappers not harming German hosta...</td>\n",
       "      <td>four german tourist kidnap mountain tribesman ...</td>\n",
       "      <td>GCAT</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>476845</td>\n",
       "      <td>476845newsML.xml</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  \\\n",
       "0              Care Group Inc Q4 shr loss vs profit.   \n",
       "1  China economy to grow at more than 10 pct in 1...   \n",
       "2    Africans say global debt relief plan too rigid.   \n",
       "3          Argentine export grain prices - March 31.   \n",
       "4  Yemen says kidnappers not harming German hosta...   \n",
       "\n",
       "                                                text bip:topics  \\\n",
       "0  ( 000 's omit ) year end decemb 31 ( audit ) 1...        C15   \n",
       "1  china see econom growth 10 percent year , infl...        E11   \n",
       "2  african financi leader criticis monday new int...        E51   \n",
       "3  export offer/bid price u.s. dollar per tonn , ...        M14   \n",
       "4  four german tourist kidnap mountain tribesman ...       GCAT   \n",
       "\n",
       "  dc.date.published  itemID     XML_File_Name  \n",
       "0        1997-03-31  476242  476242newsML.xml  \n",
       "1        1997-03-31  476768  476768newsML.xml  \n",
       "2        1997-03-31  476074  476074newsML.xml  \n",
       "3        1997-03-31  476597  476597newsML.xml  \n",
       "4        1997-03-31  476845  476845newsML.xml  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawDataFrame.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n"
     ]
    }
   ],
   "source": [
    "# PERFORMING CLUSTERING ON DOCUMENTS\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "def clustering(clusterDataFrame):\n",
    "    docVecList = []\n",
    "    textData = clusterDataFrame[\"text\"]\n",
    "    bipTopics = clusterDataFrame[\"bip:topics\"]\n",
    "\n",
    "    # The goal of doc2vec is to create a numeric representation of a document\n",
    "    tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(textData)]\n",
    "    max_epochs = 20\n",
    "    # vec_size is used to set dimension of the vector. below vec size indicates the representation\n",
    "    # of document in 20 components\n",
    "    vec_size = 20\n",
    "    alpha = 0.025 #Learning rate\n",
    "    model = Doc2Vec(size=vec_size, alpha=alpha, min_alpha=0.00025, min_count=1, dm=1)\n",
    "    #dm defines the training algorithm. If dm=1 means ‘distributed memory’ (PV-DM) and dm =0 means ‘distributed bag of words’\n",
    "    # (PV-DBOW). Distributed Memory model preserves the word order in a document whereas Distributed Bag of words just uses the bag of\n",
    "    # words approach, which doesn’t preserve any word order.\n",
    "    model.build_vocab(tagged_data)\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train(tagged_data,\n",
    "                    total_examples=model.corpus_count,\n",
    "                    epochs=model.iter)\n",
    "        # decrease the learning rate\n",
    "        model.alpha -= 0.0002\n",
    "        # fix the learning rate, no decay\n",
    "        model.min_alpha = model.alpha\n",
    "    for i in range((len(tagged_data))):\n",
    "        docVecList.append(model.docvecs[i])\n",
    "\n",
    "\n",
    "    featureDataFrame = pd.DataFrame(data=docVecList)\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    scaledFeatureArray = min_max_scaler.fit_transform(featureDataFrame)\n",
    "    scaledFeatureDataFrame = pd.DataFrame(data=scaledFeatureArray)\n",
    "    km = KMeans(n_clusters=10) #TAKEN 10 CLUSTERS\n",
    "    km.fit(scaledFeatureDataFrame)\n",
    "    clusters = km.labels_\n",
    "    scaledFeatureDataFrame['cluster_ID'] = clusters\n",
    "    scaledFeatureDataFrame['labels'] = bipTopics\n",
    "    # clusterQuality(clusters,featureData,featureDataFrame)\n",
    "    # return FeatureDataFrame\n",
    "    return scaledFeatureDataFrame,clusters,featureDataFrame\n",
    "\n",
    "receivedClusterDataframe, receivedClusters, receivedFeatureData = clustering(rawDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>cluster_ID</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.384479</td>\n",
       "      <td>0.545660</td>\n",
       "      <td>0.544152</td>\n",
       "      <td>0.606544</td>\n",
       "      <td>0.519817</td>\n",
       "      <td>0.432251</td>\n",
       "      <td>0.430371</td>\n",
       "      <td>0.369594</td>\n",
       "      <td>0.463116</td>\n",
       "      <td>0.589322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.626902</td>\n",
       "      <td>0.520464</td>\n",
       "      <td>0.361071</td>\n",
       "      <td>0.555195</td>\n",
       "      <td>0.587612</td>\n",
       "      <td>0.581672</td>\n",
       "      <td>0.092237</td>\n",
       "      <td>0.234211</td>\n",
       "      <td>9</td>\n",
       "      <td>C15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.553251</td>\n",
       "      <td>0.345231</td>\n",
       "      <td>0.626031</td>\n",
       "      <td>0.628948</td>\n",
       "      <td>0.430824</td>\n",
       "      <td>0.735184</td>\n",
       "      <td>0.547479</td>\n",
       "      <td>0.436808</td>\n",
       "      <td>0.487193</td>\n",
       "      <td>0.825580</td>\n",
       "      <td>...</td>\n",
       "      <td>0.693457</td>\n",
       "      <td>0.606522</td>\n",
       "      <td>0.247260</td>\n",
       "      <td>0.462505</td>\n",
       "      <td>0.497276</td>\n",
       "      <td>0.423199</td>\n",
       "      <td>0.338295</td>\n",
       "      <td>0.522921</td>\n",
       "      <td>3</td>\n",
       "      <td>E11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.462176</td>\n",
       "      <td>0.434969</td>\n",
       "      <td>0.626863</td>\n",
       "      <td>0.565658</td>\n",
       "      <td>0.632039</td>\n",
       "      <td>0.600619</td>\n",
       "      <td>0.482731</td>\n",
       "      <td>0.537192</td>\n",
       "      <td>0.322891</td>\n",
       "      <td>0.665864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.616268</td>\n",
       "      <td>0.508939</td>\n",
       "      <td>0.196305</td>\n",
       "      <td>0.592415</td>\n",
       "      <td>0.403272</td>\n",
       "      <td>0.473848</td>\n",
       "      <td>0.240483</td>\n",
       "      <td>0.582853</td>\n",
       "      <td>6</td>\n",
       "      <td>E51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.355043</td>\n",
       "      <td>0.370010</td>\n",
       "      <td>0.691268</td>\n",
       "      <td>0.447398</td>\n",
       "      <td>0.359245</td>\n",
       "      <td>0.484694</td>\n",
       "      <td>0.596686</td>\n",
       "      <td>0.448343</td>\n",
       "      <td>0.385433</td>\n",
       "      <td>0.757725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.584557</td>\n",
       "      <td>0.685953</td>\n",
       "      <td>0.316508</td>\n",
       "      <td>0.493998</td>\n",
       "      <td>0.577861</td>\n",
       "      <td>0.373372</td>\n",
       "      <td>0.323239</td>\n",
       "      <td>0.227906</td>\n",
       "      <td>0</td>\n",
       "      <td>M14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.481164</td>\n",
       "      <td>0.427268</td>\n",
       "      <td>0.782236</td>\n",
       "      <td>0.713602</td>\n",
       "      <td>0.543709</td>\n",
       "      <td>0.779958</td>\n",
       "      <td>0.716761</td>\n",
       "      <td>0.445762</td>\n",
       "      <td>0.472132</td>\n",
       "      <td>0.691579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.520577</td>\n",
       "      <td>0.689512</td>\n",
       "      <td>0.281714</td>\n",
       "      <td>0.250167</td>\n",
       "      <td>0.360557</td>\n",
       "      <td>0.611949</td>\n",
       "      <td>0.345340</td>\n",
       "      <td>0.575590</td>\n",
       "      <td>1</td>\n",
       "      <td>GCAT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.384479  0.545660  0.544152  0.606544  0.519817  0.432251  0.430371   \n",
       "1  0.553251  0.345231  0.626031  0.628948  0.430824  0.735184  0.547479   \n",
       "2  0.462176  0.434969  0.626863  0.565658  0.632039  0.600619  0.482731   \n",
       "3  0.355043  0.370010  0.691268  0.447398  0.359245  0.484694  0.596686   \n",
       "4  0.481164  0.427268  0.782236  0.713602  0.543709  0.779958  0.716761   \n",
       "\n",
       "          7         8         9  ...        12        13        14        15  \\\n",
       "0  0.369594  0.463116  0.589322  ...  0.626902  0.520464  0.361071  0.555195   \n",
       "1  0.436808  0.487193  0.825580  ...  0.693457  0.606522  0.247260  0.462505   \n",
       "2  0.537192  0.322891  0.665864  ...  0.616268  0.508939  0.196305  0.592415   \n",
       "3  0.448343  0.385433  0.757725  ...  0.584557  0.685953  0.316508  0.493998   \n",
       "4  0.445762  0.472132  0.691579  ...  0.520577  0.689512  0.281714  0.250167   \n",
       "\n",
       "         16        17        18        19  cluster_ID  labels  \n",
       "0  0.587612  0.581672  0.092237  0.234211           9     C15  \n",
       "1  0.497276  0.423199  0.338295  0.522921           3     E11  \n",
       "2  0.403272  0.473848  0.240483  0.582853           6     E51  \n",
       "3  0.577861  0.373372  0.323239  0.227906           0     M14  \n",
       "4  0.360557  0.611949  0.345340  0.575590           1    GCAT  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "receivedClusterDataframe.head() # PRINING THE DATAFRAME AFTER CLUSTERING WITH cluster_id INCLUDED AS COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DIVIDING THE DATAFRAME ACCORDING TO  CLUSTERS\n",
    "def clusterProcessing(receivedClusterDataframe):\n",
    "    uniqueClusterIDs = receivedClusterDataframe.cluster_ID.unique()\n",
    "    uniqueClusterIDs.sort()\n",
    "    for id in uniqueClusterIDs:\n",
    "        clusterWiseDF = receivedClusterDataframe.loc[receivedClusterDataframe['cluster_ID'] == id]\n",
    "        clusterDataframeList.append(clusterWiseDF)\n",
    "    return clusterDataframeList\n",
    "\n",
    "clusterDataframeList = clusterProcessing(receivedClusterDataframe)\n",
    "frameListforEnhance = clusterDataframeList[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[             0         1         2         3         4         5         6  \\\n",
       " 3     0.355043  0.370010  0.691268  0.447398  0.359245  0.484694  0.596686   \n",
       " 12    0.377760  0.326406  0.823562  0.564738  0.600728  0.596619  0.725821   \n",
       " 13    0.577574  0.228895  0.475024  0.431306  0.414522  0.712279  0.723252   \n",
       " 15    0.484375  0.551437  0.608820  0.598027  0.484617  0.688766  0.730665   \n",
       " 16    0.280024  0.251981  0.534861  0.570321  0.489268  0.691137  0.561637   \n",
       " ...        ...       ...       ...       ...       ...       ...       ...   \n",
       " 5078  0.512711  0.171011  0.630404  0.447109  0.599412  0.512521  0.669042   \n",
       " 5088  0.421803  0.309460  0.691877  0.483705  0.378172  0.526841  0.465823   \n",
       " 5099  0.282320  0.582900  0.257226  0.463995  0.386089  0.524770  0.989509   \n",
       " 5111  0.339258  0.396760  0.720945  0.558241  0.446837  0.685397  0.653604   \n",
       " 5112  0.367857  0.359108  0.683303  0.450575  0.417432  0.522953  0.605992   \n",
       " \n",
       "              7         8         9  ...        12        13        14  \\\n",
       " 3     0.448343  0.385433  0.757725  ...  0.584557  0.685953  0.316508   \n",
       " 12    0.497450  0.455694  0.657902  ...  0.643847  0.794596  0.453783   \n",
       " 13    0.222498  0.553993  0.776264  ...  0.538837  0.763585  0.352249   \n",
       " 15    0.365432  0.508101  0.821873  ...  0.627424  0.892046  0.283820   \n",
       " 16    0.419794  0.595820  0.719360  ...  0.751303  0.667995  0.313201   \n",
       " ...        ...       ...       ...  ...       ...       ...       ...   \n",
       " 5078  0.515678  0.418849  0.751737  ...  0.812620  0.888375  0.487488   \n",
       " 5088  0.355356  0.542605  0.885451  ...  0.567386  0.711082  0.372814   \n",
       " 5099  0.759897  0.595682  0.471895  ...  0.576762  0.892398  0.112005   \n",
       " 5111  0.438977  0.432649  0.656704  ...  0.560162  0.707361  0.286407   \n",
       " 5112  0.478448  0.396730  0.736820  ...  0.583134  0.724168  0.309553   \n",
       " \n",
       "             15        16        17        18        19  cluster_ID  labels  \n",
       " 3     0.493998  0.577861  0.373372  0.323239  0.227906           0     M14  \n",
       " 12    0.691868  0.302957  0.222933  0.392572  0.134225           0     M14  \n",
       " 13    0.624812  0.694505  0.351307  0.214770  0.333712           0     C21  \n",
       " 15    0.591084  0.587030  0.184261  0.253564  0.363083           0     C31  \n",
       " 16    0.544269  0.577063  0.476190  0.311861  0.431837           0    E512  \n",
       " ...        ...       ...       ...       ...       ...         ...     ...  \n",
       " 5078  0.666826  0.411460  0.263750  0.376424  0.266048           0     M14  \n",
       " 5088  0.651910  0.581602  0.324230  0.208695  0.497493           0     E51  \n",
       " 5099  0.144393  0.526868  0.326291  0.818195  0.292049           0     C24  \n",
       " 5111  0.527404  0.507400  0.383384  0.258975  0.308232           0     M14  \n",
       " 5112  0.492851  0.585232  0.389527  0.323758  0.209134           0     M14  \n",
       " \n",
       " [543 rows x 22 columns],\n",
       "              0         1         2         3         4         5         6  \\\n",
       " 4     0.481164  0.427268  0.782236  0.713602  0.543709  0.779958  0.716761   \n",
       " 11    0.444801  0.285213  0.651675  0.374970  0.583163  0.786886  0.529391   \n",
       " 14    0.571516  0.399026  0.469958  0.641708  0.277191  0.551260  0.666452   \n",
       " 33    0.709092  0.402676  0.528202  0.575609  0.319078  0.503771  0.589739   \n",
       " 39    0.598559  0.490432  0.604335  0.703627  0.483950  0.378468  0.526386   \n",
       " ...        ...       ...       ...       ...       ...       ...       ...   \n",
       " 5049  0.417609  0.303209  0.733436  0.580570  0.492426  0.704996  0.623842   \n",
       " 5058  0.571665  0.386375  0.615261  0.563477  0.303005  0.743448  0.624271   \n",
       " 5064  0.592735  0.304428  0.679831  0.603750  0.525849  0.685961  0.774986   \n",
       " 5081  0.404278  0.329222  0.632569  0.663294  0.566151  0.503861  0.581768   \n",
       " 5115  0.557270  0.281879  0.543759  0.719519  0.339280  0.345040  0.638797   \n",
       " \n",
       "              7         8         9  ...        12        13        14  \\\n",
       " 4     0.445762  0.472132  0.691579  ...  0.520577  0.689512  0.281714   \n",
       " 11    0.479067  0.324201  0.596889  ...  0.465791  0.766163  0.356528   \n",
       " 14    0.421592  0.490435  0.576270  ...  0.236967  0.600180  0.260779   \n",
       " 33    0.377284  0.325454  0.825647  ...  0.495146  0.642084  0.227325   \n",
       " 39    0.303072  0.470285  0.713044  ...  0.481879  0.790935  0.198047   \n",
       " ...        ...       ...       ...  ...       ...       ...       ...   \n",
       " 5049  0.353513  0.495736  0.550478  ...  0.328471  0.683959  0.275754   \n",
       " 5058  0.382664  0.403791  0.817521  ...  0.366505  0.756918  0.437945   \n",
       " 5064  0.388053  0.558323  0.825786  ...  0.492804  0.673937  0.279884   \n",
       " 5081  0.303098  0.478722  0.652984  ...  0.446925  0.654470  0.117756   \n",
       " 5115  0.440965  0.377762  0.711342  ...  0.307086  0.633035  0.159832   \n",
       " \n",
       "             15        16        17        18        19  cluster_ID  labels  \n",
       " 4     0.250167  0.360557  0.611949  0.345340  0.575590           1    GCAT  \n",
       " 11    0.323285  0.470764  0.311176  0.273178  0.518451           1     C33  \n",
       " 14    0.304060  0.367988  0.608156  0.190926  0.381222           1    GCAT  \n",
       " 33    0.220531  0.469152  0.406793  0.253456  0.472987           1    GCAT  \n",
       " 39    0.156897  0.579358  0.642161  0.212914  0.575603           1    GCAT  \n",
       " ...        ...       ...       ...       ...       ...         ...     ...  \n",
       " 5049  0.404888  0.501288  0.515800  0.355166  0.338119           1    GCAT  \n",
       " 5058  0.271272  0.286058  0.106571  0.392090  0.465663           1    GCAT  \n",
       " 5064  0.260278  0.415843  0.311401  0.387661  0.310044           1    GCAT  \n",
       " 5081  0.158966  0.460146  0.664234  0.522206  0.411340           1     C42  \n",
       " 5115  0.328763  0.280816  0.628504  0.440993  0.341252           1    GCAT  \n",
       " \n",
       " [426 rows x 22 columns],\n",
       "              0         1         2         3         4         5         6  \\\n",
       " 55    0.483114  0.398389  0.837735  0.694209  0.428690  0.556430  0.663349   \n",
       " 63    0.509926  0.471719  0.850913  0.726644  0.360449  0.323718  0.676672   \n",
       " 66    0.534331  0.445619  0.669470  0.714368  0.355146  0.684195  0.599232   \n",
       " 102   0.453951  0.530458  0.843833  0.681794  0.410843  0.476347  0.537517   \n",
       " 110   0.519577  0.445086  0.737205  0.568697  0.433030  0.447347  0.566218   \n",
       " ...        ...       ...       ...       ...       ...       ...       ...   \n",
       " 5070  0.484644  0.716388  0.721653  0.731743  0.319955  0.438632  0.766826   \n",
       " 5090  0.526301  0.385602  0.733122  0.673084  0.426115  0.438159  0.578763   \n",
       " 5098  0.544200  0.474179  0.890780  0.515417  0.452383  0.588383  0.614174   \n",
       " 5105  0.495183  0.510862  0.800910  0.721728  0.327817  0.606274  0.540476   \n",
       " 5108  0.575145  0.368544  0.727365  0.578085  0.202613  0.522769  0.603479   \n",
       " \n",
       "              7         8         9  ...        12        13        14  \\\n",
       " 55    0.541299  0.415125  0.680993  ...  0.464263  0.650739  0.247854   \n",
       " 63    0.477030  0.274063  0.611214  ...  0.625023  0.421204  0.280055   \n",
       " 66    0.314702  0.481815  0.871681  ...  0.492922  0.574231  0.310408   \n",
       " 102   0.314493  0.416610  0.553836  ...  0.375735  0.554169  0.159810   \n",
       " 110   0.359978  0.279140  0.515661  ...  0.491479  0.550808  0.249407   \n",
       " ...        ...       ...       ...  ...       ...       ...       ...   \n",
       " 5070  0.469357  0.402413  0.632725  ...  0.533770  0.728046  0.214824   \n",
       " 5090  0.314111  0.336663  0.597454  ...  0.448540  0.560086  0.253214   \n",
       " 5098  0.312998  0.392456  0.654523  ...  0.526235  0.497802  0.204201   \n",
       " 5105  0.378747  0.402124  0.667414  ...  0.367974  0.687182  0.206815   \n",
       " 5108  0.377159  0.406365  0.828105  ...  0.386442  0.714587  0.346221   \n",
       " \n",
       "             15        16        17        18        19  cluster_ID  labels  \n",
       " 55    0.345388  0.502903  0.458238  0.303691  0.402158           2    GCAT  \n",
       " 63    0.502464  0.345229  0.554826  0.178293  0.474782           2    GCAT  \n",
       " 66    0.518023  0.423800  0.263746  0.260724  0.231439           2    GCAT  \n",
       " 102   0.318522  0.533370  0.529459  0.483000  0.413178           2    GCAT  \n",
       " 110   0.558871  0.560056  0.457485  0.302159  0.454361           2     E51  \n",
       " ...        ...       ...       ...       ...       ...         ...     ...  \n",
       " 5070  0.544492  0.454120  0.378623  0.368360  0.271408           2    GCAT  \n",
       " 5090  0.345462  0.410551  0.581977  0.151392  0.409854           2    GCAT  \n",
       " 5098  0.373970  0.461818  0.365132  0.282684  0.340134           2    GCAT  \n",
       " 5105  0.372861  0.506586  0.498942  0.465552  0.460291           2    GCAT  \n",
       " 5108  0.427080  0.423611  0.201332  0.204708  0.437462           2    GCAT  \n",
       " \n",
       " [299 rows x 22 columns],\n",
       "              0         1         2         3         4         5         6  \\\n",
       " 1     0.553251  0.345231  0.626031  0.628948  0.430824  0.735184  0.547479   \n",
       " 5     0.389249  0.349516  0.641024  0.621363  0.567397  0.731189  0.547527   \n",
       " 7     0.454691  0.377924  0.672790  0.602412  0.376724  0.726213  0.527585   \n",
       " 17    0.408835  0.368843  0.694757  0.655231  0.419171  0.821820  0.457681   \n",
       " 24    0.627360  0.514918  0.551935  0.769847  0.679344  0.771615  0.516048   \n",
       " ...        ...       ...       ...       ...       ...       ...       ...   \n",
       " 5087  0.366179  0.313510  0.630972  0.676621  0.520981  0.762063  0.502925   \n",
       " 5091  0.422139  0.474485  0.596617  0.531909  0.573256  0.811906  0.548365   \n",
       " 5100  0.416930  0.525016  0.604936  0.796377  0.601425  0.729816  0.492767   \n",
       " 5103  0.350759  0.487040  0.657087  0.768603  0.584496  0.720324  0.466718   \n",
       " 5113  0.326786  0.434876  0.413565  0.644688  0.535191  0.796849  0.385670   \n",
       " \n",
       "              7         8         9  ...        12        13        14  \\\n",
       " 1     0.436808  0.487193  0.825580  ...  0.693457  0.606522  0.247260   \n",
       " 5     0.395021  0.329294  0.642231  ...  0.525809  0.673795  0.294747   \n",
       " 7     0.354657  0.421702  0.667995  ...  0.601915  0.671035  0.297714   \n",
       " 17    0.297275  0.455095  0.623835  ...  0.709371  0.625673  0.355236   \n",
       " 24    0.421872  0.311802  0.696670  ...  0.651144  0.717899  0.231212   \n",
       " ...        ...       ...       ...  ...       ...       ...       ...   \n",
       " 5087  0.315108  0.507757  0.671027  ...  0.519460  0.627167  0.225189   \n",
       " 5091  0.562513  0.423664  0.639958  ...  0.661187  0.610876  0.345669   \n",
       " 5100  0.265758  0.311157  0.612890  ...  0.635072  0.656970  0.325506   \n",
       " 5103  0.323518  0.393582  0.580011  ...  0.559014  0.719997  0.360351   \n",
       " 5113  0.653772  0.285438  0.537828  ...  0.657030  0.492536  0.369605   \n",
       " \n",
       "             15        16        17        18        19  cluster_ID  labels  \n",
       " 1     0.462505  0.497276  0.423199  0.338295  0.522921           3     E11  \n",
       " 5     0.414572  0.555144  0.424366  0.238233  0.347316           3     C15  \n",
       " 7     0.516089  0.571588  0.514097  0.322281  0.294907           3     C15  \n",
       " 17    0.354636  0.474215  0.535993  0.237961  0.307830           3     C18  \n",
       " 24    0.473974  0.494826  0.407731  0.412666  0.402267           3     C11  \n",
       " ...        ...       ...       ...       ...       ...         ...     ...  \n",
       " 5087  0.303476  0.509873  0.340092  0.310433  0.317590           3     C41  \n",
       " 5091  0.531375  0.650736  0.162388  0.142251  0.399731           3     C15  \n",
       " 5100  0.408120  0.389637  0.358210  0.310702  0.345101           3     C18  \n",
       " 5103  0.441760  0.277204  0.473503  0.367660  0.284699           3     C15  \n",
       " 5113  0.395019  0.405057  0.425367  0.282037  0.413547           3     E51  \n",
       " \n",
       " [1296 rows x 22 columns],\n",
       "              0         1         2         3         4         5         6  \\\n",
       " 6     0.405268  0.123698  0.583091  0.582148  0.666016  0.471918  0.555628   \n",
       " 30    0.534467  0.351532  0.122599  0.587088  0.294000  0.428597  0.453666   \n",
       " 52    0.392883  0.383030  0.507327  0.795795  0.322413  0.428440  0.476966   \n",
       " 58    0.562330  0.301718  0.640986  0.587190  0.389470  0.619535  0.579649   \n",
       " 61    0.359281  0.398188  0.510933  0.601987  0.502684  0.565465  0.492637   \n",
       " ...        ...       ...       ...       ...       ...       ...       ...   \n",
       " 5075  0.454771  0.198292  0.332797  0.655499  0.393866  0.553576  0.546173   \n",
       " 5085  0.374813  0.531709  0.580089  0.546715  0.397246  0.428759  0.438257   \n",
       " 5096  0.451514  0.348063  0.405052  0.571716  0.450304  0.506151  0.592404   \n",
       " 5102  0.523286  0.378087  0.520936  0.503378  0.594847  0.519704  0.520175   \n",
       " 5117  0.563245  0.351735  0.642561  0.576364  0.437511  0.470980  0.614228   \n",
       " \n",
       "              7         8         9  ...        12        13        14  \\\n",
       " 6     0.413164  0.397819  0.629456  ...  0.214203  0.555697  0.332025   \n",
       " 30    0.155004  0.325858  0.672674  ...  0.472525  0.581237  0.199379   \n",
       " 52    0.363284  0.405490  0.607551  ...  0.443729  0.595322  0.181555   \n",
       " 58    0.307501  0.356776  0.713032  ...  0.524405  0.597150  0.251472   \n",
       " 61    0.295270  0.317916  0.651155  ...  0.601547  0.610405  0.138519   \n",
       " ...        ...       ...       ...  ...       ...       ...       ...   \n",
       " 5075  0.247125  0.255439  0.792510  ...  0.626700  0.569008  0.261855   \n",
       " 5085  0.393151  0.445585  0.700827  ...  0.439319  0.622957  0.087862   \n",
       " 5096  0.279445  0.543778  0.592862  ...  0.497592  0.744167  0.080313   \n",
       " 5102  0.210099  0.289828  0.546320  ...  0.469483  0.736689  0.336461   \n",
       " 5117  0.330329  0.366135  0.727460  ...  0.253408  0.670835  0.284345   \n",
       " \n",
       "             15        16        17        18        19  cluster_ID  labels  \n",
       " 6     0.454607  0.552755  0.371588  0.311247  0.491756           4    GCAT  \n",
       " 30    0.618171  0.406548  0.366154  0.488534  0.131110           4     E21  \n",
       " 52    0.492286  0.340805  0.501512  0.296207  0.189983           4     C13  \n",
       " 58    0.593503  0.496685  0.430646  0.145270  0.316134           4    GCAT  \n",
       " 61    0.279842  0.535863  0.471990  0.198894  0.315776           4    GCAT  \n",
       " ...        ...       ...       ...       ...       ...         ...     ...  \n",
       " 5075  0.432669  0.721079  0.570453  0.215956  0.329038           4     C12  \n",
       " 5085  0.478616  0.740741  0.485715  0.249271  0.376138           4    GCAT  \n",
       " 5096  0.553863  0.695393  0.635570  0.230151  0.353225           4     C13  \n",
       " 5102  0.534126  0.574956  0.497140  0.306137  0.238277           4    GCAT  \n",
       " 5117  0.367319  0.767671  0.370791  0.285910  0.412395           4    GCAT  \n",
       " \n",
       " [296 rows x 22 columns],\n",
       "              0         1         2         3         4         5         6  \\\n",
       " 10    0.379682  0.411684  0.453847  0.757589  0.647423  0.658854  0.546586   \n",
       " 21    0.245058  0.385486  0.412445  0.682775  0.502945  0.721798  0.441286   \n",
       " 22    0.398707  0.366262  0.710785  0.707408  0.571720  0.690145  0.441455   \n",
       " 23    0.559220  0.359983  0.618504  0.713829  0.537770  0.656932  0.544623   \n",
       " 25    0.365005  0.258952  0.399256  0.646072  0.451267  0.629257  0.318657   \n",
       " ...        ...       ...       ...       ...       ...       ...       ...   \n",
       " 5080  0.434528  0.446928  0.626668  0.636830  0.361899  0.573722  0.534466   \n",
       " 5083  0.370467  0.386137  0.611391  0.694772  0.431806  0.743535  0.450126   \n",
       " 5089  0.245493  0.358965  0.751123  0.716221  0.518648  0.473968  0.439994   \n",
       " 5093  0.521168  0.448404  0.497768  0.700983  0.438628  0.591208  0.432370   \n",
       " 5097  0.428015  0.356200  0.489937  0.611426  0.534692  0.678163  0.427501   \n",
       " \n",
       "              7         8         9  ...        12        13        14  \\\n",
       " 10    0.419833  0.257669  0.468239  ...  0.590619  0.677077  0.213107   \n",
       " 21    0.275339  0.544217  0.559857  ...  0.439942  0.739389  0.257949   \n",
       " 22    0.336557  0.463238  0.666016  ...  0.623256  0.655911  0.361767   \n",
       " 23    0.401912  0.358728  0.624002  ...  0.662118  0.740380  0.244706   \n",
       " 25    0.325050  0.363095  0.731585  ...  0.464409  0.662310  0.304850   \n",
       " ...        ...       ...       ...  ...       ...       ...       ...   \n",
       " 5080  0.342953  0.344912  0.700059  ...  0.749328  0.624394  0.178114   \n",
       " 5083  0.306003  0.371730  0.583976  ...  0.563821  0.567934  0.290920   \n",
       " 5089  0.427552  0.643768  0.707767  ...  0.749031  0.668416  0.228680   \n",
       " 5093  0.051400  0.302715  0.601877  ...  0.581181  0.645395  0.420537   \n",
       " 5097  0.350856  0.391591  0.664832  ...  0.490848  0.628892  0.251417   \n",
       " \n",
       "             15        16        17        18        19  cluster_ID  labels  \n",
       " 10    0.645786  0.536919  0.419257  0.381911  0.252672           5     C17  \n",
       " 21    0.724416  0.618397  0.314984  0.199925  0.240150           5     C15  \n",
       " 22    0.580645  0.577450  0.434409  0.204564  0.273201           5     C15  \n",
       " 23    0.585041  0.571257  0.486606  0.417151  0.251003           5     C15  \n",
       " 25    0.711509  0.483018  0.474850  0.185726  0.398429           5     C11  \n",
       " ...        ...       ...       ...       ...       ...         ...     ...  \n",
       " 5080  0.597406  0.552889  0.440978  0.353478  0.257952           5     C13  \n",
       " 5083  0.627148  0.648307  0.431942  0.328589  0.323083           5     C18  \n",
       " 5089  0.731167  0.606859  0.391186  0.315663  0.321034           5     C15  \n",
       " 5093  0.746886  0.599240  0.453414  0.206741  0.251888           5     C41  \n",
       " 5097  0.548525  0.526956  0.406983  0.194196  0.301750           5     E21  \n",
       " \n",
       " [755 rows x 22 columns],\n",
       "              0         1         2         3         4         5         6  \\\n",
       " 2     0.462176  0.434969  0.626863  0.565658  0.632039  0.600619  0.482731   \n",
       " 9     0.441038  0.451633  0.537846  0.639695  0.468282  0.546915  0.634177   \n",
       " 82    0.569093  0.583119  0.575866  0.581898  0.534487  0.711022  0.543122   \n",
       " 84    0.394931  0.431550  0.697315  0.584677  0.542400  0.539170  0.487057   \n",
       " 98    0.494906  0.462011  0.622074  0.591080  0.573693  0.612148  0.461270   \n",
       " ...        ...       ...       ...       ...       ...       ...       ...   \n",
       " 5086  0.353527  0.516872  0.514476  0.795199  0.656344  0.621771  0.467851   \n",
       " 5094  0.310463  0.443124  0.599319  0.477323  0.569040  0.502700  0.604294   \n",
       " 5101  0.552000  0.450710  0.634630  0.635097  0.570403  0.607639  0.473822   \n",
       " 5104  0.478583  0.511086  0.559469  0.766518  0.735767  0.627341  0.459143   \n",
       " 5110  0.444247  0.308719  0.609783  0.580890  0.456403  0.523103  0.615700   \n",
       " \n",
       "              7         8         9  ...        12        13        14  \\\n",
       " 2     0.537192  0.322891  0.665864  ...  0.616268  0.508939  0.196305   \n",
       " 9     0.413924  0.168826  0.600358  ...  0.653164  0.564866  0.336358   \n",
       " 82    0.407793  0.389813  0.688990  ...  0.728434  0.763834  0.147751   \n",
       " 84    0.201429  0.391453  0.685221  ...  0.545647  0.801236  0.212982   \n",
       " 98    0.250769  0.360957  0.712612  ...  0.636702  0.586283  0.080686   \n",
       " ...        ...       ...       ...  ...       ...       ...       ...   \n",
       " 5086  0.328278  0.633575  0.779087  ...  0.494763  0.820376  0.250153   \n",
       " 5094  0.430085  0.354286  0.641166  ...  0.681123  0.731487  0.156909   \n",
       " 5101  0.480628  0.516418  0.787909  ...  0.516154  0.716866  0.122269   \n",
       " 5104  0.363055  0.422878  0.746516  ...  0.652061  0.653624  0.136897   \n",
       " 5110  0.355685  0.435208  0.624085  ...  0.740618  0.674210  0.164667   \n",
       " \n",
       "             15        16        17        18        19  cluster_ID  labels  \n",
       " 2     0.592415  0.403272  0.473848  0.240483  0.582853           6     E51  \n",
       " 9     0.473860  0.373945  0.388522  0.473118  0.268432           6     M13  \n",
       " 82    0.405563  0.447611  0.453519  0.397986  0.425931           6     C13  \n",
       " 84    0.418481  0.633700  0.389991  0.368086  0.273396           6     C13  \n",
       " 98    0.248129  0.432963  0.522170  0.345406  0.297656           6    GCAT  \n",
       " ...        ...       ...       ...       ...       ...         ...     ...  \n",
       " 5086  0.346463  0.480625  0.306431  0.368129  0.264461           6     C15  \n",
       " 5094  0.350218  0.748688  0.346309  0.181405  0.429798           6     C13  \n",
       " 5101  0.476422  0.591073  0.588387  0.288811  0.576280           6     E21  \n",
       " 5104  0.291051  0.541925  0.571350  0.313932  0.478484           6     C18  \n",
       " 5110  0.489530  0.463050  0.550963  0.363890  0.283437           6     M14  \n",
       " \n",
       " [407 rows x 22 columns],\n",
       "              0         1         2         3         4         5         6  \\\n",
       " 28    0.502952  0.222967  0.578673  0.578904  0.564677  0.658152  0.525971   \n",
       " 29    0.485912  0.342293  0.847453  0.602646  0.445303  0.455343  0.496962   \n",
       " 37    0.354274  0.416097  0.766202  0.741037  0.454994  0.500866  0.396661   \n",
       " 42    0.362499  0.354504  0.685437  0.627103  0.615487  0.628602  0.474331   \n",
       " 48    0.275317  0.157428  0.650389  0.660572  0.573589  0.539050  0.494764   \n",
       " ...        ...       ...       ...       ...       ...       ...       ...   \n",
       " 5041  0.445681  0.341295  0.617332  0.685518  0.567015  0.638203  0.610485   \n",
       " 5046  0.351304  0.305592  0.758917  0.740498  0.491334  0.569630  0.525532   \n",
       " 5050  0.476707  0.336508  0.670306  0.603258  0.578978  0.713333  0.564314   \n",
       " 5106  0.467410  0.456113  0.666401  0.718117  0.521025  0.368932  0.483705   \n",
       " 5116  0.216642  0.368320  0.713559  0.768660  0.546146  0.498825  0.271443   \n",
       " \n",
       "              7         8         9  ...        12        13        14  \\\n",
       " 28    0.343476  0.296800  0.655887  ...  0.678129  0.701969  0.513238   \n",
       " 29    0.288644  0.443585  0.606391  ...  0.803672  0.686629  0.279027   \n",
       " 37    0.177421  0.456097  0.622426  ...  0.748597  0.654118  0.326316   \n",
       " 42    0.245468  0.393613  0.605152  ...  0.701078  0.623567  0.289040   \n",
       " 48    0.233111  0.495663  0.762286  ...  0.599672  0.679073  0.254116   \n",
       " ...        ...       ...       ...  ...       ...       ...       ...   \n",
       " 5041  0.367864  0.230551  0.528337  ...  0.661984  0.566649  0.362402   \n",
       " 5046  0.384241  0.516363  0.594844  ...  0.766838  0.546569  0.258290   \n",
       " 5050  0.359755  0.324734  0.616998  ...  0.676426  0.620937  0.379618   \n",
       " 5106  0.531338  0.440128  0.763250  ...  0.755176  0.746393  0.196031   \n",
       " 5116  0.253629  0.475021  0.711658  ...  0.689989  0.706015  0.358957   \n",
       " \n",
       "             15        16        17        18        19  cluster_ID  labels  \n",
       " 28    0.298823  0.362570  0.362539  0.360876  0.267265           7     M13  \n",
       " 29    0.569035  0.415223  0.467491  0.347926  0.322711           7     M11  \n",
       " 37    0.540618  0.274607  0.400553  0.494841  0.405797           7     M11  \n",
       " 42    0.436600  0.353298  0.482149  0.371372  0.124092           7     C15  \n",
       " 48    0.534947  0.621297  0.461861  0.394161  0.372057           7     M12  \n",
       " ...        ...       ...       ...       ...       ...         ...     ...  \n",
       " 5041  0.391950  0.292146  0.408045  0.483410  0.234385           7     M12  \n",
       " 5046  0.648777  0.296764  0.373607  0.372596  0.436704           7     M11  \n",
       " 5050  0.572088  0.315729  0.426009  0.318126  0.309562           7     E21  \n",
       " 5106  0.627693  0.341644  0.279427  0.385627  0.404060           7     M13  \n",
       " 5116  0.354363  0.345809  0.421387  0.322558  0.419657           7     M11  \n",
       " \n",
       " [586 rows x 22 columns],\n",
       "              0         1         2         3         4         5         6  \\\n",
       " 59    0.695100  0.635254  0.441867  0.806967  0.607698  0.589548  0.654253   \n",
       " 76    0.392659  0.292738  0.417829  0.769068  0.514946  0.311577  0.515673   \n",
       " 120   0.599881  0.411348  0.588321  0.678503  0.668722  0.444333  0.593688   \n",
       " 142   0.284099  0.374002  0.718007  0.801207  0.562379  0.230726  0.372512   \n",
       " 149   0.504294  0.665529  0.773250  0.811605  0.687162  0.450172  0.646634   \n",
       " ...        ...       ...       ...       ...       ...       ...       ...   \n",
       " 5026  0.207507  0.403739  0.841398  0.748325  0.630518  0.253454  0.348813   \n",
       " 5068  0.221085  0.275274  0.383031  0.681719  0.570536  0.276062  0.493297   \n",
       " 5079  0.370499  0.402490  0.691998  0.619089  0.569112  0.294898  0.438800   \n",
       " 5109  0.129492  0.419468  0.745193  0.731582  0.549266  0.062770  0.456228   \n",
       " 5114  0.358479  0.400278  0.244843  0.736086  0.821937  0.297940  0.471526   \n",
       " \n",
       "              7         8         9  ...        12        13        14  \\\n",
       " 59    0.586668  0.585360  0.100075  ...  0.320072  0.841478  0.282491   \n",
       " 76    0.634384  0.685606  0.509075  ...  0.476665  0.743692  0.312415   \n",
       " 120   0.412070  0.773688  0.236172  ...  0.328829  0.576385  0.375299   \n",
       " 142   0.437933  0.672666  0.613339  ...  0.320684  0.683787  0.300416   \n",
       " 149   0.584527  0.466205  0.288954  ...  0.480342  0.709557  0.283115   \n",
       " ...        ...       ...       ...  ...       ...       ...       ...   \n",
       " 5026  0.478857  0.737843  0.451518  ...  0.470850  0.564887  0.256744   \n",
       " 5068  0.675038  0.353616  0.734487  ...  0.577421  0.847479  0.551484   \n",
       " 5079  0.605964  0.606897  0.435582  ...  0.371440  0.618702  0.412250   \n",
       " 5109  0.526424  0.643332  0.527223  ...  0.331247  0.808605  0.338573   \n",
       " 5114  0.808709  0.462089  0.387378  ...  0.485608  0.433162  0.363418   \n",
       " \n",
       "             15        16        17        18        19  cluster_ID  labels  \n",
       " 59    0.346750  0.583860  0.648950  0.818428  0.411120           8    GCAT  \n",
       " 76    0.400343  0.428822  0.524986  0.336140  0.107620           8    GCAT  \n",
       " 120   0.354210  0.402538  0.393371  0.305506  0.082907           8    GCAT  \n",
       " 142   0.539202  0.335231  0.536474  0.246747  0.177403           8    GCAT  \n",
       " 149   0.408076  0.369180  0.397028  0.300487  0.296615           8    GCAT  \n",
       " ...        ...       ...       ...       ...       ...         ...     ...  \n",
       " 5026  0.383639  0.453985  0.507006  0.276464  0.351370           8    GCAT  \n",
       " 5068  0.298446  0.418996  0.555248  0.312652  0.234580           8     E12  \n",
       " 5079  0.304825  0.446638  0.473525  0.145623  0.278395           8    GCAT  \n",
       " 5109  0.586679  0.200546  0.464967  0.313755  0.305143           8    GCAT  \n",
       " 5114  0.259192  0.224009  0.694489  0.302230  0.187619           8    GCAT  \n",
       " \n",
       " [152 rows x 22 columns],\n",
       "              0         1         2         3         4         5         6  \\\n",
       " 0     0.384479  0.545660  0.544152  0.606544  0.519817  0.432251  0.430371   \n",
       " 8     0.356620  0.582218  0.662807  0.612930  0.582559  0.434908  0.397366   \n",
       " 18    0.469593  0.457825  0.770996  0.567677  0.563073  0.573142  0.451610   \n",
       " 20    0.384746  0.480535  0.346249  0.740910  0.554116  0.526879  0.344220   \n",
       " 31    0.384796  0.515358  0.473890  0.387177  0.512580  0.541241  0.000000   \n",
       " ...        ...       ...       ...       ...       ...       ...       ...   \n",
       " 5071  0.308799  0.320808  0.605428  0.576789  0.454702  0.515526  0.475661   \n",
       " 5084  0.476985  0.450279  0.569909  0.445870  0.501079  0.453051  0.431505   \n",
       " 5092  0.299234  0.697911  0.546600  0.504190  0.442610  0.437780  0.380872   \n",
       " 5095  0.334182  0.477689  0.552472  0.622807  0.457076  0.625155  0.463131   \n",
       " 5107  0.378223  0.373557  0.555480  0.646257  0.521701  0.598018  0.357747   \n",
       " \n",
       "              7         8         9  ...        12        13        14  \\\n",
       " 0     0.369594  0.463116  0.589322  ...  0.626902  0.520464  0.361071   \n",
       " 8     0.346587  0.506880  0.559496  ...  0.561137  0.642734  0.397719   \n",
       " 18    0.313226  0.449621  0.629903  ...  0.612673  0.628820  0.413239   \n",
       " 20    0.337562  0.346652  0.505915  ...  0.578408  0.721063  0.311004   \n",
       " 31    0.482543  0.654044  0.681937  ...  0.621418  0.138999  0.709529   \n",
       " ...        ...       ...       ...  ...       ...       ...       ...   \n",
       " 5071  0.384851  0.342355  0.490552  ...  0.414916  0.628766  0.365520   \n",
       " 5084  0.602692  0.338112  0.733847  ...  0.806319  0.461387  0.348839   \n",
       " 5092  0.307369  0.561389  0.568230  ...  0.572463  0.756844  0.374920   \n",
       " 5095  0.266932  0.431321  0.614667  ...  0.561260  0.582947  0.326245   \n",
       " 5107  0.264702  0.468428  0.606773  ...  0.624120  0.638723  0.329103   \n",
       " \n",
       "             15        16        17        18        19  cluster_ID  labels  \n",
       " 0     0.555195  0.587612  0.581672  0.092237  0.234211           9     C15  \n",
       " 8     0.515792  0.750446  0.501691  0.207348  0.232459           9     C15  \n",
       " 18    0.660363  0.710234  0.504716  0.204577  0.270872           9     C15  \n",
       " 20    0.556378  0.722969  0.494842  0.269849  0.176696           9     C15  \n",
       " 31    0.476287  0.601393  0.722070  0.352229  0.980079           9     E71  \n",
       " ...        ...       ...       ...       ...       ...         ...     ...  \n",
       " 5071  0.651106  0.509893  0.538284  0.340717  0.321323           9     E21  \n",
       " 5084  0.465311  0.648343  0.450680  0.265493  0.435794           9     E71  \n",
       " 5092  0.628054  0.885385  0.372189  0.265227  0.297311           9     C15  \n",
       " 5095  0.590336  0.674613  0.458150  0.073913  0.391320           9     C15  \n",
       " 5107  0.559997  0.725373  0.520743  0.150343  0.457647           9     C15  \n",
       " \n",
       " [358 rows x 22 columns]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusterDataframeList # Cluster data is maintained in the respective index of the List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Networks\n",
      "========================================\n",
      "Accuracy :  0.6788990825688074\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C11       0.00      0.00      0.00         1\n",
      "         C12       0.00      0.00      0.00         0\n",
      "         C13       0.29      0.67      0.40         3\n",
      "         C15       0.00      0.00      0.00         0\n",
      "         C21       0.38      0.25      0.30        12\n",
      "         C22       0.00      0.00      0.00         0\n",
      "         C24       0.38      0.38      0.38         8\n",
      "         C31       0.17      0.14      0.15         7\n",
      "         C33       0.00      0.00      0.00         2\n",
      "         E13       0.00      0.00      0.00         1\n",
      "         E51       0.67      0.50      0.57         4\n",
      "        E512       0.00      0.00      0.00         0\n",
      "        GCAT       1.00      1.00      1.00         1\n",
      "         M12       0.00      0.00      0.00         1\n",
      "         M13       0.00      0.00      0.00         0\n",
      "         M14       0.91      0.90      0.91        69\n",
      "\n",
      "    accuracy                           0.68       109\n",
      "   macro avg       0.24      0.24      0.23       109\n",
      "weighted avg       0.70      0.68      0.68       109\n",
      "\n",
      "SVC\n",
      "========================================\n",
      "Accuracy :  0.7558139534883721\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C12       0.00      0.00      0.00         0\n",
      "         C13       0.00      0.00      0.00         0\n",
      "         C15       0.00      0.00      0.00         0\n",
      "         C16       0.00      0.00      0.00         0\n",
      "         C21       0.00      0.00      0.00         0\n",
      "         C24       0.00      0.00      0.00         0\n",
      "         C42       0.00      0.00      0.00         0\n",
      "         E41       0.00      0.00      0.00         0\n",
      "         E51       0.00      0.00      0.00         0\n",
      "        GCAT       1.00      0.76      0.86        86\n",
      "\n",
      "    accuracy                           0.76        86\n",
      "   macro avg       0.10      0.08      0.09        86\n",
      "weighted avg       1.00      0.76      0.86        86\n",
      "\n",
      "Decision Trees\n",
      "========================================\n",
      "Accuracy :  0.8166666666666667\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C13       0.00      0.00      0.00         0\n",
      "         C24       0.00      0.00      0.00         0\n",
      "         C33       0.00      0.00      0.00         0\n",
      "         C42       0.00      0.00      0.00         0\n",
      "         E21       0.00      0.00      0.00         0\n",
      "         E51       0.00      0.00      0.00         0\n",
      "         G15       0.00      0.00      0.00         0\n",
      "        GCAT       1.00      0.82      0.90        60\n",
      "\n",
      "    accuracy                           0.82        60\n",
      "   macro avg       0.12      0.10      0.11        60\n",
      "weighted avg       1.00      0.82      0.90        60\n",
      "\n",
      "Random Forest\n",
      "========================================\n",
      "Accuracy :  0.31153846153846154\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C11       0.00      0.00      0.00         0\n",
      "         C13       0.00      0.00      0.00         0\n",
      "         C14       0.00      0.00      0.00         0\n",
      "         C15       1.00      0.31      0.48       260\n",
      "        C151       0.00      0.00      0.00         0\n",
      "         C16       0.00      0.00      0.00         0\n",
      "         C17       0.00      0.00      0.00         0\n",
      "         C18       0.00      0.00      0.00         0\n",
      "         C21       0.00      0.00      0.00         0\n",
      "         C22       0.00      0.00      0.00         0\n",
      "         C24       0.00      0.00      0.00         0\n",
      "         C31       0.00      0.00      0.00         0\n",
      "         C33       0.00      0.00      0.00         0\n",
      "         C41       0.00      0.00      0.00         0\n",
      "         C42       0.00      0.00      0.00         0\n",
      "        CCAT       0.00      0.00      0.00         0\n",
      "         E11       0.00      0.00      0.00         0\n",
      "         E12       0.00      0.00      0.00         0\n",
      "         E13       0.00      0.00      0.00         0\n",
      "         E21       0.00      0.00      0.00         0\n",
      "         E31       0.00      0.00      0.00         0\n",
      "         E51       0.00      0.00      0.00         0\n",
      "         E61       0.00      0.00      0.00         0\n",
      "        GCAT       0.00      0.00      0.00         0\n",
      "         M11       0.00      0.00      0.00         0\n",
      "         M12       0.00      0.00      0.00         0\n",
      "         M13       0.00      0.00      0.00         0\n",
      "         M14       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.31       260\n",
      "   macro avg       0.04      0.01      0.02       260\n",
      "weighted avg       1.00      0.31      0.48       260\n",
      "\n",
      "KNearestNeighbors\n",
      "=========================================\n",
      "Accuracy :  0.6\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C11       0.00      0.00      0.00         0\n",
      "         C12       0.88      0.39      0.54        18\n",
      "         C13       0.75      0.55      0.63        11\n",
      "         C15       0.00      0.00      0.00         0\n",
      "         C21       0.00      0.00      0.00         0\n",
      "         C23       0.00      0.00      0.00         0\n",
      "         C24       0.00      0.00      0.00         0\n",
      "         C31       0.00      0.00      0.00         0\n",
      "         C33       0.00      0.00      0.00         0\n",
      "         C41       0.00      0.00      0.00         0\n",
      "         C42       0.00      0.00      0.00         0\n",
      "         E12       1.00      1.00      1.00         1\n",
      "         E21       1.00      0.78      0.88         9\n",
      "         E41       0.00      0.00      0.00         0\n",
      "         G15       0.00      0.00      0.00         0\n",
      "        GCAT       0.79      0.71      0.75        21\n",
      "        GDIP       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.60        60\n",
      "   macro avg       0.26      0.20      0.22        60\n",
      "weighted avg       0.84      0.60      0.69        60\n",
      "\n",
      "Guassian Naive Bayes\n",
      "=========================================\n",
      "Accuracy :  0.4966887417218543\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C11       0.29      0.25      0.27        16\n",
      "         C12       0.25      0.33      0.29         3\n",
      "         C13       0.50      0.33      0.40         6\n",
      "         C14       0.00      0.00      0.00         1\n",
      "         C15       0.76      0.67      0.71        51\n",
      "        C151       0.00      0.00      0.00         0\n",
      "         C16       0.00      0.00      0.00         0\n",
      "         C17       0.42      0.62      0.50         8\n",
      "         C18       0.36      0.53      0.43        19\n",
      "         C21       0.00      0.00      0.00         4\n",
      "         C22       0.25      0.33      0.29         3\n",
      "         C24       0.67      0.17      0.27        12\n",
      "         C31       0.00      0.00      0.00         0\n",
      "         C33       0.62      0.56      0.59         9\n",
      "         C41       0.71      0.42      0.53        12\n",
      "         C42       0.00      0.00      0.00         0\n",
      "         E11       0.00      0.00      0.00         0\n",
      "         E21       1.00      0.80      0.89         5\n",
      "         E31       0.00      0.00      0.00         0\n",
      "         E51       0.00      0.00      0.00         0\n",
      "        GCAT       0.00      0.00      0.00         0\n",
      "         M11       1.00      1.00      1.00         2\n",
      "         M14       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.50       151\n",
      "   macro avg       0.30      0.26      0.27       151\n",
      "weighted avg       0.58      0.50      0.52       151\n",
      "\n",
      "Guassian Multinomial Naive Bayes\n",
      "=========================================\n",
      "Accuracy :  0.2682926829268293\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C13       0.00      0.00      0.00         0\n",
      "         C15       0.00      0.00      0.00         0\n",
      "         C21       0.00      0.00      0.00         0\n",
      "         C24       0.00      0.00      0.00         0\n",
      "         C31       0.00      0.00      0.00         0\n",
      "         C42       0.00      0.00      0.00         0\n",
      "         E11       0.00      0.00      0.00         0\n",
      "         E12       0.00      0.00      0.00         0\n",
      "         E13       0.00      0.00      0.00         0\n",
      "         E14       0.00      0.00      0.00         0\n",
      "        E141       0.00      0.00      0.00         0\n",
      "         E21       0.00      0.00      0.00         0\n",
      "         E41       0.00      0.00      0.00         0\n",
      "         E51       0.00      0.00      0.00         0\n",
      "         E71       0.00      0.00      0.00         0\n",
      "        GCAT       1.00      0.27      0.42        82\n",
      "         M12       0.00      0.00      0.00         0\n",
      "         M13       0.00      0.00      0.00         0\n",
      "         M14       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.27        82\n",
      "   macro avg       0.05      0.01      0.02        82\n",
      "weighted avg       1.00      0.27      0.42        82\n",
      "\n",
      "Stochastic Gradient Descent\n",
      "=========================================\n",
      "Accuracy :  0.559322033898305\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C13       0.00      0.00      0.00         0\n",
      "         C15       0.09      0.50      0.15         2\n",
      "         C17       0.00      0.00      0.00         0\n",
      "         C18       0.00      0.00      0.00         2\n",
      "         C21       0.00      0.00      0.00         0\n",
      "         C31       0.00      0.00      0.00         1\n",
      "         C41       0.00      0.00      0.00         0\n",
      "         E12       0.00      0.00      0.00         0\n",
      "         E21       0.00      0.00      0.00         0\n",
      "         E51       0.00      0.00      0.00         0\n",
      "        G151       0.00      0.00      0.00         0\n",
      "         M11       0.81      0.78      0.79        37\n",
      "         M12       0.07      1.00      0.12         1\n",
      "         M13       1.00      0.47      0.64        75\n",
      "         M14       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.56       118\n",
      "   macro avg       0.13      0.18      0.11       118\n",
      "weighted avg       0.89      0.56      0.66       118\n",
      "\n",
      "ADA-Boost\n",
      "=========================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.967741935483871\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        GCAT       1.00      0.97      0.98        31\n",
      "         M14       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.97        31\n",
      "   macro avg       0.50      0.48      0.49        31\n",
      "weighted avg       1.00      0.97      0.98        31\n",
      "\n",
      "Bagging\n",
      "=========================================\n",
      "Accuracy :  0.875\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C13       0.00      0.00      0.00         0\n",
      "         C15       1.00      0.90      0.95        48\n",
      "         C17       0.00      0.00      0.00         2\n",
      "         C18       0.00      0.00      0.00         0\n",
      "         C31       0.00      0.00      0.00         0\n",
      "         E21       0.93      0.93      0.93        14\n",
      "         E51       0.00      0.00      0.00         0\n",
      "         E71       1.00      1.00      1.00         5\n",
      "         M12       0.50      1.00      0.67         1\n",
      "         M14       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.88        72\n",
      "   macro avg       0.44      0.43      0.42        72\n",
      "weighted avg       0.95      0.88      0.91        72\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def applyClassifier(receivedDFList):\n",
    "    for df in receivedDFList:\n",
    "        clusterNumber = df['cluster_ID'].unique()\n",
    "        clusterNumber = clusterNumber[0]\n",
    "        \n",
    "        Xtr, Xte, Ytr, Yte, target = trainTestSplit(df)\n",
    "        #\n",
    "        if clusterNumber == 0:\n",
    "            # Artificial Neural Network(ANN) uses the processing of the brain as a basis \n",
    "            # to develop algorithms that can be used to model complex patterns and prediction problems.\n",
    "            print(\"Neural Networks\")\n",
    "            print(\"========================================\")\n",
    "            neuralNetwork_model = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                                                hidden_layer_sizes=(60,), random_state=1, max_iter=500)\n",
    "            trainedClassifier = neuralNetwork_model.fit(Xtr, Ytr)\n",
    "#SVM constructs a hyperplane in multidimensional space to separate different classes. \n",
    "#SVM generates optimal hyperplane in an iterative manner, which is used to minimize an error.\n",
    "        elif clusterNumber == 1:\n",
    "            print(\"SVC\")\n",
    "            print(\"========================================\")\n",
    "            SVC_model = SVC(kernel='sigmoid', gamma=0.1, C=0.1)\n",
    "            trainedClassifier = SVC_model.fit(Xtr, Ytr)\n",
    "\n",
    "        elif clusterNumber == 2:\n",
    "            #A decision tree is a flowchart-like structure in which each internal node represents a “test” on an attribute\n",
    "            # each branch represents the outcome of the test, and each leaf node represents a class label.\n",
    "            #The paths from root to leaf represent classification rules.\n",
    "            print(\"Decision Trees\")\n",
    "            print(\"========================================\")\n",
    "            decionTree_model = DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_split=0.2,\n",
    "                                                      min_samples_leaf=0.2)\n",
    "            trainedClassifier = decionTree_model.fit(Xtr, Ytr)\n",
    "            \n",
    "        # Random forest is an ensemble method in which a classifier is constructed by combining several different Independent base classifiers.\n",
    "\n",
    "        elif clusterNumber == 3:\n",
    "            print(\"Random Forest\")\n",
    "            print(\"========================================\")\n",
    "            randomForest_model = RandomForestClassifier(n_estimators=10, max_depth=3, min_samples_split=0.4,\n",
    "                                                        min_samples_leaf=0.2)\n",
    "            trainedClassifier = randomForest_model.fit(Xtr, Ytr)\n",
    "        # KNN is a non-parametric and lazy learning algorithm. Non-parametric means there is no assumption for underlying data distribution.\n",
    "        elif clusterNumber == 4:\n",
    "            print(\"KNearestNeighbors\")\n",
    "            print(\"=========================================\")\n",
    "            KNN_model = KNeighborsClassifier(n_neighbors=5)\n",
    "            trainedClassifier = KNN_model.fit(Xtr, Ytr)\n",
    "        # Naive Bayes is a classification algorithm for binary (two-class) and multi-class classification problems.\n",
    "        elif clusterNumber == 5:\n",
    "            print(\"Guassian Naive Bayes\")\n",
    "            print(\"=========================================\")\n",
    "            GNB_model = GaussianNB()\n",
    "            trainedClassifier = GNB_model.fit(Xtr, Ytr)\n",
    "        # Multinomial Naive Bayes calculates likelihood to be count of an word/token \n",
    "\n",
    "        elif clusterNumber == 6:\n",
    "            print(\"Guassian Multinomial Naive Bayes\")\n",
    "            print(\"=========================================\")\n",
    "            GMNB_model = MultinomialNB()\n",
    "            trainedClassifier = GMNB_model.fit(Xtr, Ytr)\n",
    "\n",
    "        # We use only a single training example for calculation of gradient and update parameters.\n",
    "        elif clusterNumber == 7:\n",
    "            print(\"Stochastic Gradient Descent\")\n",
    "            print(\"=========================================\")\n",
    "            SGD_model = SGDClassifier(loss='modified_huber',shuffle=True,random_state=101)\n",
    "            trainedClassifier = SGD_model.fit(Xtr, Ytr)\n",
    "\n",
    "        # AdaBoost is a popular boosting technique which helps you combine multiple “weak classifiers” into a single “strong classifier”.\n",
    "        elif clusterNumber == 8:\n",
    "            print(\"ADA-Boost\")\n",
    "            print(\"=========================================\")\n",
    "            ADAB_model = AdaBoostClassifier(n_estimators=50,learning_rate=1)\n",
    "            trainedClassifier = ADAB_model.fit(Xtr, Ytr)\n",
    "        #It’s a sub-class of ensemble machine learning algorithms wherein we use multiple weak models and aggregate the predictions we get from each of them to get the final prediction. \n",
    "        elif clusterNumber == 9:\n",
    "            print(\"Bagging\")\n",
    "            print(\"=========================================\")\n",
    "            Bagging_model = BaggingClassifier(n_estimators=50)\n",
    "            trainedClassifier = Bagging_model.fit(Xtr, Ytr)\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(\"end\")\n",
    "            break\n",
    "            \n",
    "        calculateMetrics(trainedClassifier, Xte, Yte)\n",
    "    return target\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def trainTestSplit(dataFrame):\n",
    "    target = dataFrame['labels']\n",
    "    splitDF = dataFrame.iloc[:,:-1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(splitDF, target, test_size=0.2, random_state=101)\n",
    "    return X_train, X_test, y_train, y_test, target\n",
    "\n",
    "def calculateMetrics(trainedReceivedClassifier, XtestData, YtestData):\n",
    "    predictor = trainedReceivedClassifier.predict(XtestData)\n",
    "    confusionMatrix = confusion_matrix(predictor, YtestData)\n",
    "    accuracy = accuracy_score(predictor, YtestData)\n",
    "    ClassificationReport = classification_report(predictor, YtestData)\n",
    "    print(\"Accuracy : \", accuracy)\n",
    "    print(\"Classification Report :\")\n",
    "    print(ClassificationReport)\n",
    "    \n",
    "    \n",
    "target = applyClassifier(frameListforEnhance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Quality Scores\n",
      "with ground truth labels\n",
      "==========================\n",
      "Adjusted Rand index\n",
      "0.15939412457482804\n",
      "Mutual Information based scores\n",
      "0.2851248324630502\n",
      "Homogeneity score\n",
      "0.29529300393510155\n",
      "completeness score\n",
      "0.38985191589515333\n",
      "V Measure Score\n",
      "0.3360472799332894\n",
      "Fowlkes-Mallows scores\n",
      "0.2579530261148003\n",
      "\n",
      "\n",
      "\n",
      "without ground truth labels\n",
      "============================\n",
      "silhouette\n",
      "0.06415026681464157\n"
     ]
    }
   ],
   "source": [
    "def clusterQuality(receivedclusters,receivedfeatureData,receivedfeatureDataFrame):\n",
    "    receivedbipTopics = receivedfeatureDataFrame.labels\n",
    "    # The knowledge of ground truth classes is known and hence the measure taking those classes into consideration is used\n",
    "    # To compare both evaluation based on ground truth tables and independent of them, silhouette score is used\n",
    "    # which resulted in biased scoring.\n",
    "    \n",
    "    \n",
    "    print(\"Cluster Quality Scores\")\n",
    "    print(\"with ground truth labels\")\n",
    "    print(\"==========================\")\n",
    "    \n",
    "    #The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples \n",
    "    #and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.\n",
    "    print(\"Adjusted Rand index\")\n",
    "    print(metrics.adjusted_rand_score(receivedbipTopics, receivedclusters))\n",
    "    \n",
    "    \n",
    "    #The Mutual Information is a measure of the similarity between two labels of the same data.\n",
    "    #This Mutualinformation score is useful to check whether the clustering algorithm meets an important requirement:\n",
    "    #a cluster should contain only samples belonging to a single class.\n",
    "    print(\"Mutual Information based scores\")\n",
    "    print(metrics.adjusted_mutual_info_score(receivedbipTopics, receivedclusters))\n",
    "    \n",
    "    \n",
    "    #A perfectly homogeneous clustering is one where each cluster has data-points belonging to the same class label. \n",
    "    #Homogeneity describes the closeness of the clustering algorithm to this perfection.\n",
    "    print(\"Homogeneity score\")\n",
    "    print(metrics.homogeneity_score(receivedbipTopics, receivedclusters))\n",
    "    \n",
    "    \n",
    "    #Completness score purpose is to provide a piece of information about the assignment of samples belonging to the same class.\n",
    "    #More precisely, a good clustering algorithm should assign all samples with the same true label to the same cluster.\n",
    "    print(\"completeness score\")\n",
    "    print(metrics.completeness_score(receivedbipTopics, receivedclusters))\n",
    "    \n",
    "    \n",
    "    #The V-Measure is defined as the harmonic mean of homogeneity and completeness of the clustering\n",
    "    # No assumption is made on the cluster structure: can be used to compare clustering algorithms such as k-means which assumes isotropic \n",
    "    # blob shapes with results of spectral clustering algorithms which can find cluster with “folded” shapes.\n",
    "    print(\"V Measure Score\")\n",
    "    print(metrics.v_measure_score(receivedbipTopics, receivedclusters))\n",
    "    \n",
    "    \n",
    "    #The Fowlkes-Mallows Score is an evaluation metric to evaluate the similarity among clusterings obtained after applying different clustering algorithms. \n",
    "    print(\"Fowlkes-Mallows scores\")\n",
    "    print(metrics.fowlkes_mallows_score(receivedbipTopics, receivedclusters))\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"without ground truth labels\")\n",
    "    print(\"============================\")\n",
    "    print(\"silhouette\")\n",
    "    print(metrics.silhouette_score(receivedfeatureData, receivedclusters, metric='euclidean'))\n",
    "    \n",
    "clusterQuality(receivedClusters, receivedFeatureData, receivedClusterDataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhancing Features using AutoEncoder..........\n",
      "Enhancing Features using AutoEncoder..........\n",
      "Enhancing Features using AutoEncoder..........\n",
      "Enhancing Features using AutoEncoder..........\n",
      "Enhancing Features using AutoEncoder..........\n",
      "Enhancing Features using AutoEncoder..........\n",
      "Enhancing Features using AutoEncoder..........\n",
      "Enhancing Features using AutoEncoder..........\n",
      "Enhancing Features using AutoEncoder..........\n",
      "Enhancing Features using AutoEncoder..........\n",
      "[            0         1         2         3         4         5         6  \\\n",
      "0    0.385522  0.361760  0.664825  0.492591  0.445960  0.558124  0.647401   \n",
      "1    0.379567  0.276782  0.727491  0.484896  0.448895  0.519895  0.643062   \n",
      "2    0.387488  0.295412  0.709807  0.487679  0.450273  0.519474  0.635800   \n",
      "3    0.382898  0.298773  0.712127  0.488030  0.449913  0.529625  0.641925   \n",
      "4    0.390469  0.328210  0.686808  0.490915  0.452032  0.535644  0.634825   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "538  0.374833  0.260890  0.739983  0.484255  0.444987  0.513518  0.649433   \n",
      "539  0.392351  0.305857  0.699920  0.488528  0.451735  0.518196  0.629216   \n",
      "540  0.357543  0.440813  0.609853  0.492770  0.417522  0.606173  0.701500   \n",
      "541  0.386600  0.337070  0.684130  0.491341  0.450725  0.548023  0.641792   \n",
      "542  0.384546  0.357941  0.668389  0.492315  0.446247  0.558124  0.648107   \n",
      "\n",
      "            7         8         9  ...        12        13        14  \\\n",
      "0    0.432316  0.476974  0.700894  ...  0.645043  0.744142  0.332444   \n",
      "1    0.370409  0.463644  0.788153  ...  0.712035  0.814190  0.325625   \n",
      "2    0.381488  0.468503  0.771653  ...  0.698150  0.793803  0.336814   \n",
      "3    0.385078  0.467878  0.769428  ...  0.696034  0.795500  0.329754   \n",
      "4    0.404211  0.472839  0.738333  ...  0.671564  0.765161  0.341061   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "538  0.359299  0.461636  0.806286  ...  0.728308  0.829451  0.318585   \n",
      "539  0.388675  0.470071  0.759495  ...  0.688384  0.780898  0.344564   \n",
      "540  0.508610  0.490165  0.620945  ...  0.592536  0.715316  0.291545   \n",
      "541  0.411867  0.473226  0.728893  ...  0.664637  0.762909  0.334128   \n",
      "542  0.429630  0.476261  0.705152  ...  0.648050  0.748166  0.331036   \n",
      "\n",
      "           15        16        17        18        19  cluster_ID  labels  \n",
      "0    0.536435  0.503267  0.411180  0.350487  0.306913           0     M14  \n",
      "1    0.591786  0.482173  0.360363  0.281155  0.254578           0     M14  \n",
      "2    0.584539  0.493473  0.372449  0.294654  0.274811           0     C21  \n",
      "3    0.578657  0.490747  0.372429  0.296708  0.270191           0     C31  \n",
      "4    0.562354  0.498663  0.391280  0.321243  0.295752           0    E512  \n",
      "..        ...       ...       ...       ...       ...         ...     ...  \n",
      "538  0.603206  0.484680  0.350532  0.265165  0.243159           0     M14  \n",
      "539  0.580129  0.495326  0.379465  0.304299  0.286106           0     E51  \n",
      "540  0.473890  0.518196  0.450612  0.427448  0.322860           0     C24  \n",
      "541  0.552500  0.497209  0.395478  0.328666  0.294692           0     M14  \n",
      "542  0.538052  0.501944  0.408728  0.347290  0.303601           0     M14  \n",
      "\n",
      "[543 rows x 22 columns],             0         1         2         3         4         5         6  \\\n",
      "0    0.453289  0.416871  0.679488  0.629729  0.469197  0.633599  0.574928   \n",
      "1    0.459271  0.427212  0.656348  0.614401  0.469106  0.621492  0.563834   \n",
      "2    0.466658  0.430226  0.644278  0.605728  0.460929  0.610713  0.563297   \n",
      "3    0.462271  0.417086  0.667272  0.628895  0.458476  0.630235  0.567228   \n",
      "4    0.456534  0.408562  0.684205  0.642886  0.459338  0.640469  0.573496   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "421  0.459233  0.429036  0.654253  0.608978  0.468824  0.616832  0.567261   \n",
      "422  0.459281  0.425610  0.658316  0.615290  0.464640  0.624325  0.568320   \n",
      "423  0.451813  0.412832  0.681627  0.635046  0.464398  0.639973  0.577916   \n",
      "424  0.447036  0.417830  0.679480  0.627850  0.475463  0.629168  0.577954   \n",
      "425  0.460543  0.433187  0.649115  0.600606  0.472891  0.604028  0.566995   \n",
      "\n",
      "            7         8         9  ...        12        13        14  \\\n",
      "0    0.370482  0.433745  0.701684  ...  0.460698  0.668034  0.253923   \n",
      "1    0.382325  0.441492  0.681356  ...  0.466787  0.649449  0.283515   \n",
      "2    0.387194  0.453532  0.673822  ...  0.466096  0.638730  0.297452   \n",
      "3    0.353598  0.439487  0.714185  ...  0.475057  0.668988  0.254176   \n",
      "4    0.337447  0.432130  0.735944  ...  0.475625  0.689351  0.227205   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "421  0.392391  0.446421  0.671272  ...  0.459849  0.643215  0.291576   \n",
      "422  0.381048  0.443077  0.684865  ...  0.462184  0.652973  0.282307   \n",
      "423  0.356831  0.433954  0.717673  ...  0.463962  0.680720  0.242245   \n",
      "424  0.371767  0.434226  0.701941  ...  0.458647  0.675217  0.247555   \n",
      "425  0.407009  0.449937  0.654780  ...  0.454739  0.632554  0.303076   \n",
      "\n",
      "           15        16        17        18        19  cluster_ID  labels  \n",
      "0    0.258122  0.370120  0.516650  0.342584  0.390050           1    GCAT  \n",
      "1    0.285548  0.391634  0.510296  0.353033  0.404392           1     C33  \n",
      "2    0.298515  0.402059  0.515694  0.358962  0.407893           1    GCAT  \n",
      "3    0.257657  0.395150  0.512737  0.309606  0.392729           1    GCAT  \n",
      "4    0.230823  0.383672  0.515382  0.289106  0.381286           1    GCAT  \n",
      "..        ...       ...       ...       ...       ...         ...     ...  \n",
      "421  0.293537  0.387736  0.514871  0.372601  0.404863           1    GCAT  \n",
      "422  0.282448  0.390029  0.510374  0.354513  0.401491           1    GCAT  \n",
      "423  0.244999  0.375488  0.514005  0.323039  0.384879           1    GCAT  \n",
      "424  0.249988  0.366591  0.518632  0.350181  0.387640           1     C42  \n",
      "425  0.305124  0.383011  0.519323  0.397405  0.408091           1    GCAT  \n",
      "\n",
      "[426 rows x 22 columns],             0         1         2         3         4         5         6  \\\n",
      "0    0.488223  0.457719  0.768739  0.624530  0.431480  0.461638  0.631564   \n",
      "1    0.488593  0.457776  0.768129  0.624495  0.431800  0.460998  0.631737   \n",
      "2    0.486832  0.452720  0.794826  0.639190  0.423090  0.457912  0.646848   \n",
      "3    0.489953  0.463158  0.737958  0.608445  0.440216  0.466262  0.614458   \n",
      "4    0.488894  0.459828  0.756324  0.617603  0.435569  0.462819  0.624995   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "294  0.487011  0.454738  0.786566  0.634124  0.426336  0.458215  0.642165   \n",
      "295  0.488825  0.459008  0.760979  0.620467  0.433749  0.462615  0.627302   \n",
      "296  0.488883  0.459264  0.761262  0.620439  0.433462  0.462841  0.627407   \n",
      "297  0.489072  0.460336  0.754337  0.616880  0.435550  0.464105  0.623217   \n",
      "298  0.486988  0.453834  0.789667  0.635810  0.424811  0.458582  0.643676   \n",
      "\n",
      "            7         8         9  ...        12        13        14  \\\n",
      "0    0.439589  0.396557  0.652119  ...  0.464031  0.624608  0.285181   \n",
      "1    0.439827  0.396150  0.651163  ...  0.464756  0.622908  0.285507   \n",
      "2    0.432325  0.384239  0.671286  ...  0.460454  0.640647  0.262182   \n",
      "3    0.446998  0.410246  0.631405  ...  0.467664  0.607537  0.312119   \n",
      "4    0.443096  0.401740  0.643113  ...  0.465601  0.617768  0.296107   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "294  0.434930  0.388016  0.664287  ...  0.461684  0.635299  0.269280   \n",
      "295  0.441488  0.399897  0.646652  ...  0.465210  0.619779  0.291983   \n",
      "296  0.441526  0.399628  0.647350  ...  0.465271  0.620303  0.291683   \n",
      "297  0.442945  0.403330  0.642421  ...  0.465411  0.616750  0.298138   \n",
      "298  0.434019  0.386858  0.667563  ...  0.460864  0.637956  0.266991   \n",
      "\n",
      "           15        16        17        18        19  cluster_ID  labels  \n",
      "0    0.486023  0.438505  0.408929  0.304731  0.382071           2    GCAT  \n",
      "1    0.486266  0.438389  0.410297  0.304372  0.382739           2    GCAT  \n",
      "2    0.484904  0.431751  0.396216  0.282636  0.367855           2    GCAT  \n",
      "3    0.487334  0.445871  0.421862  0.330003  0.397566           2    GCAT  \n",
      "4    0.487579  0.441909  0.413748  0.315199  0.388333           2     E51  \n",
      "..        ...       ...       ...       ...       ...         ...     ...  \n",
      "294  0.485767  0.434048  0.400578  0.289935  0.372631           2    GCAT  \n",
      "295  0.486409  0.440308  0.412738  0.310842  0.386201           2    GCAT  \n",
      "296  0.486241  0.440438  0.411959  0.310551  0.385925           2    GCAT  \n",
      "297  0.486622  0.441954  0.414917  0.316949  0.389415           2    GCAT  \n",
      "298  0.485375  0.433275  0.398126  0.287140  0.370721           2    GCAT  \n",
      "\n",
      "[299 rows x 22 columns],              0         1         2         3         4         5         6  \\\n",
      "0     0.408646  0.363306  0.648223  0.658925  0.516775  0.746587  0.519620   \n",
      "1     0.381012  0.366509  0.674683  0.625240  0.491040  0.693493  0.515885   \n",
      "2     0.375078  0.361099  0.682377  0.628844  0.488792  0.698052  0.515539   \n",
      "3     0.382675  0.414205  0.663624  0.664675  0.540169  0.717543  0.500026   \n",
      "4     0.449189  0.442019  0.597203  0.699426  0.589806  0.770658  0.490322   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1291  0.381033  0.435935  0.658177  0.654005  0.543393  0.695176  0.497028   \n",
      "1292  0.394775  0.420773  0.648791  0.641265  0.531793  0.692866  0.504668   \n",
      "1293  0.438100  0.450413  0.607394  0.711653  0.598544  0.775612  0.485356   \n",
      "1294  0.426971  0.405500  0.622961  0.685067  0.559014  0.760357  0.500526   \n",
      "1295  0.431045  0.375708  0.600376  0.639314  0.525335  0.730127  0.513057   \n",
      "\n",
      "             7         8         9  ...        12        13        14  \\\n",
      "0     0.393130  0.354752  0.641751  ...  0.613338  0.628765  0.313574   \n",
      "1     0.389745  0.389194  0.645647  ...  0.605401  0.659329  0.289078   \n",
      "2     0.384865  0.386485  0.651470  ...  0.609676  0.665898  0.281173   \n",
      "3     0.354071  0.428778  0.670357  ...  0.621796  0.697305  0.267032   \n",
      "4     0.373431  0.402214  0.641307  ...  0.616416  0.638521  0.324224   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "1291  0.356007  0.453909  0.666929  ...  0.613652  0.703362  0.265077   \n",
      "1292  0.374106  0.433779  0.651034  ...  0.604817  0.676528  0.287927   \n",
      "1293  0.354031  0.416533  0.656547  ...  0.624676  0.663907  0.303406   \n",
      "1294  0.377688  0.382798  0.644632  ...  0.617118  0.638702  0.317344   \n",
      "1295  0.439581  0.340239  0.591615  ...  0.591327  0.566495  0.373167   \n",
      "\n",
      "            15        16        17        18        19  cluster_ID  labels  \n",
      "0     0.518997  0.432596  0.429183  0.325998  0.320019           3     E11  \n",
      "1     0.521958  0.487808  0.445665  0.293066  0.310489           3     C15  \n",
      "2     0.524794  0.490055  0.444880  0.284854  0.302911           3     C15  \n",
      "3     0.474452  0.505568  0.402250  0.271912  0.324902           3     C18  \n",
      "4     0.459680  0.428327  0.383414  0.339791  0.373004           3     C11  \n",
      "...        ...       ...       ...       ...       ...         ...     ...  \n",
      "1291  0.462887  0.522551  0.395464  0.267408  0.334437           3     C41  \n",
      "1292  0.477807  0.504792  0.413033  0.291678  0.341007           3     C15  \n",
      "1293  0.445405  0.442797  0.371372  0.318403  0.366572           3     C18  \n",
      "1294  0.485261  0.432405  0.404152  0.331745  0.347655           3     C15  \n",
      "1295  0.530788  0.394160  0.436815  0.382334  0.356233           3     E51  \n",
      "\n",
      "[1296 rows x 22 columns],             0         1         2         3         4         5         6  \\\n",
      "0    0.403904  0.359716  0.464215  0.577164  0.444844  0.514105  0.533439   \n",
      "1    0.400729  0.354999  0.462644  0.580226  0.442581  0.515024  0.534902   \n",
      "2    0.401764  0.356607  0.463257  0.579308  0.443103  0.514551  0.534231   \n",
      "3    0.394627  0.346005  0.460876  0.584832  0.439407  0.515301  0.537003   \n",
      "4    0.396131  0.348139  0.461653  0.583331  0.440456  0.514858  0.536367   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "291  0.386900  0.334294  0.457957  0.590964  0.435018  0.516179  0.540210   \n",
      "292  0.396506  0.348868  0.461724  0.582993  0.440612  0.514751  0.536249   \n",
      "293  0.388872  0.337108  0.458598  0.589299  0.436152  0.515914  0.539503   \n",
      "294  0.396401  0.348771  0.461583  0.583252  0.440480  0.515351  0.536357   \n",
      "295  0.394739  0.346084  0.460958  0.584482  0.439595  0.515236  0.537075   \n",
      "\n",
      "            7         8         9  ...        12        13        14  \\\n",
      "0    0.302051  0.409848  0.673283  ...  0.449898  0.620973  0.257496   \n",
      "1    0.295842  0.407020  0.679832  ...  0.448163  0.625386  0.249638   \n",
      "2    0.297992  0.407797  0.677490  ...  0.448810  0.623864  0.252393   \n",
      "3    0.283626  0.400876  0.690463  ...  0.446075  0.633252  0.236006   \n",
      "4    0.286225  0.402266  0.687598  ...  0.447026  0.631300  0.239267   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "291  0.267990  0.393118  0.704474  ...  0.442560  0.643491  0.218804   \n",
      "292  0.287020  0.402797  0.686883  ...  0.447106  0.630703  0.240162   \n",
      "293  0.271587  0.395197  0.700973  ...  0.443277  0.641046  0.222892   \n",
      "294  0.287234  0.402872  0.686959  ...  0.446705  0.630842  0.240266   \n",
      "295  0.283595  0.401141  0.690116  ...  0.446054  0.633156  0.236292   \n",
      "\n",
      "           15        16        17        18        19  cluster_ID  labels  \n",
      "0    0.500443  0.579963  0.509003  0.296554  0.332825           4    GCAT  \n",
      "1    0.500983  0.582890  0.509441  0.290205  0.326519           4     E21  \n",
      "2    0.500858  0.581811  0.509375  0.292217  0.328727           4     C13  \n",
      "3    0.501492  0.589028  0.509980  0.277142  0.316667           4    GCAT  \n",
      "4    0.501246  0.587965  0.509818  0.279887  0.319251           4    GCAT  \n",
      "..        ...       ...       ...       ...       ...         ...     ...  \n",
      "291  0.502478  0.597100  0.511044  0.260994  0.303331           4     C12  \n",
      "292  0.501235  0.587709  0.509750  0.280767  0.319817           4    GCAT  \n",
      "293  0.502270  0.595265  0.510859  0.264749  0.306605           4     C13  \n",
      "294  0.501094  0.587178  0.509785  0.281261  0.319699           4    GCAT  \n",
      "295  0.501232  0.589181  0.509877  0.277329  0.316884           4    GCAT  \n",
      "\n",
      "[296 rows x 22 columns],             0         1         2         3         4         5         6  \\\n",
      "0    0.411766  0.381190  0.540692  0.660551  0.529118  0.676632  0.512607   \n",
      "1    0.400612  0.367345  0.545582  0.680524  0.532688  0.696963  0.514750   \n",
      "2    0.405413  0.373267  0.543837  0.672322  0.530992  0.688700  0.513893   \n",
      "3    0.410177  0.379076  0.541650  0.663867  0.529502  0.680089  0.513013   \n",
      "4    0.408743  0.377279  0.541905  0.666077  0.529823  0.681991  0.513149   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "750  0.410261  0.379269  0.541543  0.663686  0.529283  0.679692  0.513027   \n",
      "751  0.410544  0.379631  0.541461  0.662967  0.529159  0.679319  0.512894   \n",
      "752  0.401854  0.368964  0.545339  0.678625  0.532284  0.694673  0.514570   \n",
      "753  0.403766  0.370837  0.544456  0.675418  0.531618  0.691885  0.514221   \n",
      "754  0.412304  0.381941  0.540505  0.659715  0.528722  0.675765  0.512533   \n",
      "\n",
      "            7         8         9  ...        12        13        14  \\\n",
      "0    0.323923  0.454875  0.653466  ...  0.564379  0.665536  0.306413   \n",
      "1    0.303227  0.450937  0.672678  ...  0.572778  0.686465  0.284640   \n",
      "2    0.311716  0.452635  0.664799  ...  0.569512  0.677764  0.293576   \n",
      "3    0.320428  0.454282  0.656726  ...  0.565922  0.668959  0.302731   \n",
      "4    0.318247  0.454018  0.658911  ...  0.566782  0.671450  0.300548   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "750  0.320604  0.454431  0.656697  ...  0.565917  0.668917  0.302893   \n",
      "751  0.321212  0.454479  0.655833  ...  0.565434  0.668112  0.303843   \n",
      "752  0.305586  0.451564  0.670940  ...  0.572353  0.684232  0.286711   \n",
      "753  0.308254  0.451803  0.667820  ...  0.570684  0.681159  0.290072   \n",
      "754  0.324725  0.455212  0.652759  ...  0.564157  0.664775  0.307438   \n",
      "\n",
      "           15        16        17        18        19  cluster_ID  labels  \n",
      "0    0.644702  0.570837  0.415011  0.317696  0.310487           5     C17  \n",
      "1    0.662621  0.581237  0.404295  0.296714  0.289136           5     C15  \n",
      "2    0.655138  0.576698  0.408685  0.305351  0.297835           5     C15  \n",
      "3    0.647631  0.572428  0.413224  0.314258  0.306833           5     C15  \n",
      "4    0.649805  0.573747  0.412011  0.311907  0.304727           5     C11  \n",
      "..        ...       ...       ...       ...       ...         ...     ...  \n",
      "750  0.647562  0.572522  0.413296  0.314554  0.307001           5     C13  \n",
      "751  0.646910  0.572152  0.413610  0.315076  0.307924           5     C18  \n",
      "752  0.660871  0.579881  0.405383  0.299039  0.291174           5     C15  \n",
      "753  0.658227  0.578551  0.407059  0.301969  0.294438           5     C41  \n",
      "754  0.643910  0.570385  0.415359  0.318613  0.311452           5     E21  \n",
      "\n",
      "[755 rows x 22 columns],             0         1         2         3         4         5         6  \\\n",
      "0    0.424550  0.442988  0.594972  0.590326  0.531524  0.588669  0.521305   \n",
      "1    0.368651  0.386479  0.564827  0.608480  0.518133  0.576332  0.500215   \n",
      "2    0.469562  0.459478  0.621888  0.623177  0.560647  0.533732  0.519409   \n",
      "3    0.361309  0.336491  0.554627  0.699055  0.541021  0.423357  0.444919   \n",
      "4    0.449932  0.433425  0.594736  0.638027  0.549984  0.477694  0.490320   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "402  0.334716  0.306130  0.551929  0.732796  0.547848  0.396827  0.427601   \n",
      "403  0.374601  0.366866  0.574258  0.651495  0.538755  0.531430  0.490642   \n",
      "404  0.379894  0.377124  0.577776  0.639104  0.536038  0.549994  0.498288   \n",
      "405  0.418211  0.395491  0.584578  0.669366  0.552364  0.449905  0.470425   \n",
      "406  0.345895  0.349177  0.551503  0.638484  0.520219  0.546456  0.485849   \n",
      "\n",
      "            7         8         9  ...        12        13        14  \\\n",
      "0    0.483712  0.379569  0.675002  ...  0.676758  0.552807  0.211836   \n",
      "1    0.458350  0.398443  0.674885  ...  0.677155  0.573923  0.222306   \n",
      "2    0.397598  0.388513  0.690533  ...  0.626260  0.642166  0.177041   \n",
      "3    0.271477  0.462828  0.710742  ...  0.566330  0.753518  0.169424   \n",
      "4    0.353233  0.417335  0.676684  ...  0.589165  0.668625  0.191653   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "402  0.231827  0.469548  0.738409  ...  0.564737  0.794542  0.139066   \n",
      "403  0.370989  0.427020  0.700343  ...  0.623765  0.671189  0.192889   \n",
      "404  0.398146  0.417194  0.694682  ...  0.638761  0.644023  0.198430   \n",
      "405  0.310155  0.433632  0.695399  ...  0.576932  0.714236  0.173719   \n",
      "406  0.402084  0.427808  0.683383  ...  0.644448  0.633080  0.217634   \n",
      "\n",
      "           15        16        17        18        19  cluster_ID  labels  \n",
      "0    0.435297  0.514433  0.431056  0.345567  0.470934           6     E51  \n",
      "1    0.423971  0.541966  0.455338  0.394036  0.474023           6     M13  \n",
      "2    0.408821  0.480700  0.416582  0.325920  0.413836           6     C13  \n",
      "3    0.348271  0.528801  0.481206  0.433010  0.368033           6     C13  \n",
      "4    0.388293  0.490779  0.444831  0.375180  0.394283           6    GCAT  \n",
      "..        ...       ...       ...       ...       ...         ...     ...  \n",
      "402  0.320677  0.533495  0.488527  0.439196  0.344506           6     C15  \n",
      "403  0.404891  0.531978  0.450949  0.382645  0.429978           6     C13  \n",
      "404  0.415003  0.529871  0.446723  0.375088  0.443855           6     E21  \n",
      "405  0.368534  0.500505  0.456217  0.390228  0.377943           6     C18  \n",
      "406  0.411780  0.549887  0.466348  0.413206  0.454363           6     M14  \n",
      "\n",
      "[407 rows x 22 columns],             0         1         2         3         4         5         6  \\\n",
      "0    0.422421  0.324158  0.703785  0.616742  0.508890  0.548389  0.473446   \n",
      "1    0.414094  0.302218  0.726806  0.634067  0.511405  0.541824  0.466058   \n",
      "2    0.413148  0.299326  0.729413  0.636562  0.511340  0.541781  0.465156   \n",
      "3    0.416031  0.306732  0.722185  0.630956  0.511080  0.543390  0.467519   \n",
      "4    0.408778  0.288720  0.737403  0.643807  0.511781  0.539099  0.462063   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "581  0.440455  0.375824  0.651313  0.579752  0.506287  0.547413  0.484355   \n",
      "582  0.419779  0.318636  0.710609  0.621108  0.509874  0.547100  0.471438   \n",
      "583  0.428016  0.341819  0.687386  0.603871  0.508454  0.550543  0.477748   \n",
      "584  0.412210  0.297878  0.731604  0.637524  0.511725  0.542166  0.464858   \n",
      "585  0.400471  0.267762  0.759795  0.661494  0.514216  0.535214  0.455338   \n",
      "\n",
      "            7         8         9  ...        12        13        14  \\\n",
      "0    0.386618  0.428996  0.670308  ...  0.684017  0.630243  0.377021   \n",
      "1    0.367559  0.430081  0.697937  ...  0.707794  0.658982  0.355704   \n",
      "2    0.365257  0.429856  0.701183  ...  0.710479  0.662194  0.353721   \n",
      "3    0.371360  0.430228  0.692560  ...  0.703000  0.653650  0.360034   \n",
      "4    0.356824  0.429672  0.711911  ...  0.719153  0.673820  0.345061   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "581  0.425258  0.436268  0.611850  ...  0.632140  0.574350  0.418979   \n",
      "582  0.381884  0.428807  0.677736  ...  0.690939  0.637229  0.371184   \n",
      "583  0.400385  0.429434  0.649935  ...  0.667424  0.609339  0.391504   \n",
      "584  0.364366  0.429059  0.702779  ...  0.712305  0.663385  0.352197   \n",
      "585  0.339259  0.429566  0.736838  ...  0.741407  0.699515  0.325547   \n",
      "\n",
      "           15        16        17        18        19  cluster_ID  labels  \n",
      "0    0.521267  0.389683  0.449815  0.399594  0.354042           7     M13  \n",
      "1    0.525379  0.386250  0.442439  0.380762  0.347813           7     M11  \n",
      "2    0.525374  0.385694  0.441454  0.378756  0.347146           7     M11  \n",
      "3    0.524275  0.387274  0.443856  0.384383  0.349593           7     C15  \n",
      "4    0.526962  0.385312  0.437993  0.371074  0.345418           7     M12  \n",
      "..        ...       ...       ...       ...       ...         ...     ...  \n",
      "581  0.516584  0.405102  0.467040  0.438314  0.372585           7     M12  \n",
      "582  0.522993  0.387507  0.448031  0.394596  0.351085           7     M11  \n",
      "583  0.520325  0.391519  0.455784  0.413200  0.356888           7     E21  \n",
      "584  0.526173  0.384061  0.440820  0.377454  0.345101           7     M13  \n",
      "585  0.529907  0.379238  0.430988  0.353653  0.337044           7     M11  \n",
      "\n",
      "[586 rows x 22 columns],             0         1         2         3         4         5         6  \\\n",
      "0    0.420734  0.463216  0.543049  0.609907  0.547650  0.407276  0.513876   \n",
      "1    0.358385  0.434028  0.589272  0.678339  0.560602  0.348369  0.516087   \n",
      "2    0.400508  0.457731  0.558275  0.632629  0.554587  0.387463  0.514951   \n",
      "3    0.298639  0.380092  0.638400  0.734812  0.548506  0.292442  0.509947   \n",
      "4    0.376246  0.450796  0.577431  0.658399  0.561819  0.365450  0.515197   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "147  0.316586  0.406406  0.626495  0.716021  0.556586  0.310389  0.510315   \n",
      "148  0.365743  0.446624  0.586078  0.667314  0.562671  0.356962  0.514253   \n",
      "149  0.377538  0.452189  0.577946  0.654235  0.559000  0.367788  0.513136   \n",
      "150  0.260726  0.353490  0.669151  0.772232  0.547514  0.254966  0.507847   \n",
      "151  0.394780  0.455846  0.562311  0.637580  0.556618  0.383012  0.514511   \n",
      "\n",
      "            7         8         9  ...        12        13        14  \\\n",
      "0    0.540946  0.558725  0.459209  ...  0.452931  0.607939  0.397207   \n",
      "1    0.555254  0.592642  0.461674  ...  0.414660  0.667104  0.330425   \n",
      "2    0.552307  0.569919  0.455553  ...  0.443246  0.628534  0.377818   \n",
      "3    0.521326  0.622419  0.500972  ...  0.355618  0.707458  0.256063   \n",
      "4    0.564130  0.581355  0.453733  ...  0.432085  0.652332  0.355257   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "147  0.547940  0.610664  0.484329  ...  0.381083  0.695786  0.286203   \n",
      "148  0.567578  0.584636  0.457384  ...  0.427093  0.659256  0.347340   \n",
      "149  0.564088  0.578847  0.457762  ...  0.433282  0.647163  0.359738   \n",
      "150  0.515458  0.643971  0.515636  ...  0.322833  0.739171  0.215419   \n",
      "151  0.555718  0.571143  0.455739  ...  0.441939  0.633287  0.373410   \n",
      "\n",
      "           15        16        17        18        19  cluster_ID  labels  \n",
      "0    0.434274  0.413258  0.492380  0.409582  0.385249           8    GCAT  \n",
      "1    0.425210  0.355418  0.493122  0.346677  0.327184           8    GCAT  \n",
      "2    0.420066  0.395239  0.485436  0.385405  0.363854           8    GCAT  \n",
      "3    0.505286  0.299277  0.534239  0.306528  0.292592           8    GCAT  \n",
      "4    0.404846  0.373705  0.479088  0.357860  0.340418           8    GCAT  \n",
      "..        ...       ...       ...       ...       ...         ...     ...  \n",
      "147  0.451515  0.319183  0.504131  0.309699  0.301718           8    GCAT  \n",
      "148  0.401787  0.365719  0.477249  0.346757  0.332568           8     E12  \n",
      "149  0.406664  0.377056  0.477679  0.359114  0.344651           8    GCAT  \n",
      "150  0.528457  0.263794  0.546612  0.273498  0.263346           8    GCAT  \n",
      "151  0.414670  0.390678  0.483668  0.379060  0.358736           8    GCAT  \n",
      "\n",
      "[152 rows x 22 columns],             0         1         2         3         4         5         6  \\\n",
      "0    0.320727  0.528289  0.553144  0.579461  0.507909  0.527462  0.405493   \n",
      "1    0.313385  0.615436  0.515777  0.573703  0.510056  0.493500  0.347941   \n",
      "2    0.315907  0.476075  0.610575  0.590924  0.522663  0.583548  0.440873   \n",
      "3    0.294912  0.558481  0.523297  0.591358  0.501708  0.496651  0.372880   \n",
      "4    0.468815  0.533288  0.473737  0.478103  0.480285  0.480126  0.441309   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "353  0.334699  0.353828  0.590362  0.583188  0.497427  0.580799  0.518432   \n",
      "354  0.358786  0.451841  0.548499  0.546809  0.487282  0.518669  0.469282   \n",
      "355  0.301176  0.691036  0.457800  0.566309  0.496701  0.440954  0.295452   \n",
      "356  0.306770  0.479094  0.606612  0.596367  0.521892  0.579839  0.433889   \n",
      "357  0.307516  0.495507  0.597226  0.593812  0.520716  0.570999  0.423247   \n",
      "\n",
      "            7         8         9  ...        12        13        14  \\\n",
      "0    0.338672  0.453352  0.572674  ...  0.584087  0.638967  0.352430   \n",
      "1    0.294417  0.474484  0.555738  ...  0.593115  0.653386  0.362343   \n",
      "2    0.357869  0.458713  0.596115  ...  0.581153  0.654682  0.343328   \n",
      "3    0.306752  0.439919  0.569303  ...  0.593336  0.653952  0.334216   \n",
      "4    0.423116  0.468154  0.482002  ...  0.513493  0.472417  0.490852   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "353  0.440377  0.381817  0.585347  ...  0.531168  0.578083  0.364002   \n",
      "354  0.404536  0.419359  0.566009  ...  0.571223  0.570527  0.374688   \n",
      "355  0.253519  0.474738  0.532088  ...  0.604823  0.653163  0.363187   \n",
      "356  0.350107  0.454524  0.597805  ...  0.584370  0.660669  0.335710   \n",
      "357  0.343290  0.458555  0.593860  ...  0.586748  0.660460  0.338650   \n",
      "\n",
      "           15        16        17        18        19  cluster_ID  labels  \n",
      "0    0.575763  0.720996  0.479186  0.184936  0.295103           9     C15  \n",
      "1    0.571665  0.777001  0.476990  0.166857  0.277772           9     C15  \n",
      "2    0.579498  0.695906  0.460409  0.164794  0.295458           9     C15  \n",
      "3    0.591131  0.764074  0.487181  0.164627  0.265839           9     C15  \n",
      "4    0.531698  0.567255  0.534919  0.426787  0.416707           9     E71  \n",
      "..        ...       ...       ...       ...       ...         ...     ...  \n",
      "353  0.604549  0.583429  0.498574  0.262905  0.304741           9     E21  \n",
      "354  0.558341  0.632466  0.501470  0.277768  0.338054           9     E71  \n",
      "355  0.574409  0.823437  0.484766  0.161448  0.255303           9     C15  \n",
      "356  0.584223  0.707166  0.461053  0.156686  0.286683           9     C15  \n",
      "357  0.581652  0.716180  0.462594  0.157091  0.286545           9     C15  \n",
      "\n",
      "[358 rows x 22 columns]]\n"
     ]
    }
   ],
   "source": [
    "#Here Auto encoder is used for feature extraction.\n",
    "#Autoencoder is an unsupervised artificial neural network that learns how to efficiently compress and encode data then learns how to reconstruct the data back from the reduced encoded representation to a representation that is as close to the original input as possible.\n",
    "#Autoencoder, by design, reduces data dimensions by learning how to ignore the noise in the data.\n",
    "#It consists of 4 parts , one is encoder in which model learns how to reduce the input dimensions and compress the input data into encoded representation.\n",
    "#Bottle neck , this layer contains the compressed representation of nput data\n",
    "#Decoder , in which model learns how to recontruct the data from encoded representation to be close to the original input as possible.\n",
    "#I decided to use ReLu as the activation function for the encoding stage and Softmax for the decoding stage.\n",
    "#In here I have declared 3 hidden layers in the encoded stage and 3 hidden layers in the decoded stage.\n",
    "\n",
    "def enhancedFeatureExtraction(receivedFeatureData):\n",
    "    print(\"Enhancing Features using AutoEncoder..........\")\n",
    "    x = Input(shape=(receivedFeatureData.shape[1],))\n",
    "    # 3 hidden layers are implemented\n",
    "    hidden_1en = Dense(2048, activation='relu')(x)\n",
    "    hidden_2en = Dense(1024, activation='relu')(hidden_1en)\n",
    "    hidden_3en = Dense(512, activation='relu')(hidden_2en)\n",
    "    h = Dense(128, activation='relu')(hidden_3en)\n",
    "    hidden_1dec = Dense(512, activation='relu')(h)\n",
    "    hidden_2dec = Dense(1024, activation='relu')(hidden_1dec)\n",
    "    hidden_3dec = Dense(2048, activation='relu')(hidden_2dec)\n",
    "    r = Dense(receivedFeatureData.shape[1], activation='sigmoid')(hidden_3dec)\n",
    "    autoencoder = Model(x, r)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    Xtraut, Xteaut, Ytraut, Yteaut = train_test_split(receivedFeatureData, receivedFeatureData, test_size=0.3, random_state=101)\n",
    "    autoencoder.fit(Xtraut, Ytraut,\n",
    "                    epochs=30,\n",
    "                    batch_size=200,\n",
    "                    shuffle=True,\n",
    "                    verbose=0,\n",
    "                    validation_data=(Xteaut, Yteaut))\n",
    "    compressedData = autoencoder.predict(receivedFeatureData)\n",
    "    return compressedData\n",
    "\n",
    "for lf in clusterDataframeList:\n",
    " enhancedDFrame = lf.iloc[:, :-2]\n",
    " clusterandLabel = lf.iloc[:, -2:]\n",
    " compressedFrame = enhancedFeatureExtraction(enhancedDFrame) # PERFORMING AUTO ENCODER FOR DIFFERENT CLUSTER DATA\n",
    " compressedDataFrame = pd.DataFrame(data=compressedFrame)\n",
    " clusterandLabel.reset_index(drop=True, inplace=True)\n",
    " compressedDataFrame.reset_index(drop=True, inplace=True)\n",
    " compressedDataFrame = pd.concat([compressedDataFrame,clusterandLabel],axis=1)\n",
    " enhancedDFList.append(compressedDataFrame) \n",
    "    \n",
    "print(enhancedDFList)# PRINTING THE EXTRACTED FEATURES USING AUTO ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Networks\n",
      "========================================\n",
      "Accuracy :  0.6513761467889908\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C11       0.00      0.00      0.00         0\n",
      "         C12       0.00      0.00      0.00         0\n",
      "         C13       0.14      1.00      0.25         1\n",
      "         C15       0.00      0.00      0.00         0\n",
      "         C21       0.25      0.25      0.25         8\n",
      "         C22       0.00      0.00      0.00         0\n",
      "         C24       0.12      1.00      0.22         1\n",
      "         C31       0.00      0.00      0.00         1\n",
      "         C33       0.00      0.00      0.00         0\n",
      "         E51       0.00      0.00      0.00         0\n",
      "        E512       0.00      0.00      0.00         0\n",
      "        GCAT       0.00      0.00      0.00         0\n",
      "         M13       0.00      0.00      0.00         0\n",
      "         M14       0.99      0.68      0.81        98\n",
      "\n",
      "    accuracy                           0.65       109\n",
      "   macro avg       0.11      0.21      0.11       109\n",
      "weighted avg       0.91      0.65      0.75       109\n",
      "\n",
      "SVC\n",
      "========================================\n",
      "Accuracy :  0.7558139534883721\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C12       0.00      0.00      0.00         0\n",
      "         C13       0.00      0.00      0.00         0\n",
      "         C15       0.00      0.00      0.00         0\n",
      "         C16       0.00      0.00      0.00         0\n",
      "         C21       0.00      0.00      0.00         0\n",
      "         C24       0.00      0.00      0.00         0\n",
      "         C42       0.00      0.00      0.00         0\n",
      "         E41       0.00      0.00      0.00         0\n",
      "         E51       0.00      0.00      0.00         0\n",
      "        GCAT       1.00      0.76      0.86        86\n",
      "\n",
      "    accuracy                           0.76        86\n",
      "   macro avg       0.10      0.08      0.09        86\n",
      "weighted avg       1.00      0.76      0.86        86\n",
      "\n",
      "Decision Trees\n",
      "========================================\n",
      "Accuracy :  0.8166666666666667\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C13       0.00      0.00      0.00         0\n",
      "         C24       0.00      0.00      0.00         0\n",
      "         C33       0.00      0.00      0.00         0\n",
      "         C42       0.00      0.00      0.00         0\n",
      "         E21       0.00      0.00      0.00         0\n",
      "         E51       0.00      0.00      0.00         0\n",
      "         G15       0.00      0.00      0.00         0\n",
      "        GCAT       1.00      0.82      0.90        60\n",
      "\n",
      "    accuracy                           0.82        60\n",
      "   macro avg       0.12      0.10      0.11        60\n",
      "weighted avg       1.00      0.82      0.90        60\n",
      "\n",
      "Random Forest\n",
      "========================================\n",
      "Accuracy :  0.31153846153846154\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C11       0.00      0.00      0.00         0\n",
      "         C13       0.00      0.00      0.00         0\n",
      "         C14       0.00      0.00      0.00         0\n",
      "         C15       1.00      0.31      0.48       260\n",
      "        C151       0.00      0.00      0.00         0\n",
      "         C16       0.00      0.00      0.00         0\n",
      "         C17       0.00      0.00      0.00         0\n",
      "         C18       0.00      0.00      0.00         0\n",
      "         C21       0.00      0.00      0.00         0\n",
      "         C22       0.00      0.00      0.00         0\n",
      "         C24       0.00      0.00      0.00         0\n",
      "         C31       0.00      0.00      0.00         0\n",
      "         C33       0.00      0.00      0.00         0\n",
      "         C41       0.00      0.00      0.00         0\n",
      "         C42       0.00      0.00      0.00         0\n",
      "        CCAT       0.00      0.00      0.00         0\n",
      "         E11       0.00      0.00      0.00         0\n",
      "         E12       0.00      0.00      0.00         0\n",
      "         E13       0.00      0.00      0.00         0\n",
      "         E21       0.00      0.00      0.00         0\n",
      "         E31       0.00      0.00      0.00         0\n",
      "         E51       0.00      0.00      0.00         0\n",
      "         E61       0.00      0.00      0.00         0\n",
      "        GCAT       0.00      0.00      0.00         0\n",
      "         M11       0.00      0.00      0.00         0\n",
      "         M12       0.00      0.00      0.00         0\n",
      "         M13       0.00      0.00      0.00         0\n",
      "         M14       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.31       260\n",
      "   macro avg       0.04      0.01      0.02       260\n",
      "weighted avg       1.00      0.31      0.48       260\n",
      "\n",
      "KNearestNeighbors\n",
      "=========================================\n",
      "Accuracy :  0.26666666666666666\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C11       0.00      0.00      0.00         0\n",
      "         C12       0.12      0.14      0.13         7\n",
      "         C13       0.38      0.21      0.27        14\n",
      "         C15       0.00      0.00      0.00         0\n",
      "         C17       0.00      0.00      0.00         1\n",
      "         C21       0.00      0.00      0.00         0\n",
      "         C23       0.00      0.00      0.00         0\n",
      "         C24       0.00      0.00      0.00         0\n",
      "         C31       0.00      0.00      0.00         0\n",
      "         C33       0.00      0.00      0.00         0\n",
      "         C41       0.00      0.00      0.00         0\n",
      "         C42       0.00      0.00      0.00         0\n",
      "         E12       0.00      0.00      0.00         0\n",
      "         E21       0.29      0.22      0.25         9\n",
      "         E41       0.00      0.00      0.00         0\n",
      "         G15       0.00      0.00      0.00         0\n",
      "        GCAT       0.53      0.34      0.42        29\n",
      "        GDIP       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.27        60\n",
      "   macro avg       0.07      0.05      0.06        60\n",
      "weighted avg       0.40      0.27      0.32        60\n",
      "\n",
      "Guassian Naive Bayes\n",
      "=========================================\n",
      "Accuracy :  0.059602649006622516\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C11       0.00      0.00      0.00         0\n",
      "         C12       0.00      0.00      0.00         0\n",
      "         C13       0.00      0.00      0.00         0\n",
      "         C14       0.00      0.00      0.00        32\n",
      "         C15       0.11      0.62      0.19         8\n",
      "        C151       1.00      0.03      0.06        32\n",
      "        C152       0.00      0.00      0.00         5\n",
      "         C16       0.00      0.00      0.00         0\n",
      "         C17       0.00      0.00      0.00         0\n",
      "        C171       0.00      0.00      0.00         3\n",
      "         C18       0.00      0.00      0.00         0\n",
      "         C21       0.00      0.00      0.00         0\n",
      "         C22       0.00      0.00      0.00        10\n",
      "         C23       0.00      0.00      0.00        27\n",
      "         C24       0.00      0.00      0.00         0\n",
      "         C31       0.00      0.00      0.00         0\n",
      "         C33       0.00      0.00      0.00         0\n",
      "         C41       0.00      0.00      0.00         0\n",
      "         C42       0.00      0.00      0.00         0\n",
      "         E11       0.00      0.00      0.00         0\n",
      "         E21       0.75      0.10      0.18        29\n",
      "         E31       0.00      0.00      0.00         0\n",
      "         E51       0.00      0.00      0.00         0\n",
      "        GCAT       0.00      0.00      0.00         0\n",
      "         M11       0.00      0.00      0.00         0\n",
      "         M12       0.00      0.00      0.00         5\n",
      "         M14       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.06       151\n",
      "   macro avg       0.07      0.03      0.02       151\n",
      "weighted avg       0.36      0.06      0.06       151\n",
      "\n",
      "Guassian Multinomial Naive Bayes\n",
      "=========================================\n",
      "Accuracy :  0.2682926829268293\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C13       0.00      0.00      0.00         0\n",
      "         C15       0.00      0.00      0.00         0\n",
      "         C21       0.00      0.00      0.00         0\n",
      "         C24       0.00      0.00      0.00         0\n",
      "         C31       0.00      0.00      0.00         0\n",
      "         C42       0.00      0.00      0.00         0\n",
      "         E11       0.00      0.00      0.00         0\n",
      "         E12       0.00      0.00      0.00         0\n",
      "         E13       0.00      0.00      0.00         0\n",
      "         E14       0.00      0.00      0.00         0\n",
      "        E141       0.00      0.00      0.00         0\n",
      "         E21       0.00      0.00      0.00         0\n",
      "         E41       0.00      0.00      0.00         0\n",
      "         E51       0.00      0.00      0.00         0\n",
      "         E71       0.00      0.00      0.00         0\n",
      "        GCAT       1.00      0.27      0.42        82\n",
      "         M12       0.00      0.00      0.00         0\n",
      "         M13       0.00      0.00      0.00         0\n",
      "         M14       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.27        82\n",
      "   macro avg       0.05      0.01      0.02        82\n",
      "weighted avg       1.00      0.27      0.42        82\n",
      "\n",
      "Stochastic Gradient Descent\n",
      "=========================================\n",
      "Accuracy :  0.01694915254237288\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C13       0.00      0.00      0.00         0\n",
      "         C15       0.00      0.00      0.00         0\n",
      "         C17       0.00      0.00      0.00         0\n",
      "         C18       1.00      0.01      0.02       114\n",
      "         C21       0.00      0.00      0.00         0\n",
      "         C41       0.00      0.00      0.00         0\n",
      "         E12       0.00      0.00      0.00         0\n",
      "         E21       0.00      0.00      0.00         0\n",
      "         E51       0.00      0.00      0.00         0\n",
      "        G151       0.00      0.00      0.00         0\n",
      "         M11       0.03      0.25      0.05         4\n",
      "         M12       0.00      0.00      0.00         0\n",
      "         M13       0.00      0.00      0.00         0\n",
      "         M14       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.02       118\n",
      "   macro avg       0.07      0.02      0.00       118\n",
      "weighted avg       0.97      0.02      0.02       118\n",
      "\n",
      "ADA-Boost\n",
      "=========================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.967741935483871\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        GCAT       1.00      0.97      0.98        31\n",
      "         M14       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.97        31\n",
      "   macro avg       0.50      0.48      0.49        31\n",
      "weighted avg       1.00      0.97      0.98        31\n",
      "\n",
      "Bagging\n",
      "=========================================\n",
      "Accuracy :  0.8611111111111112\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C13       0.00      0.00      0.00         0\n",
      "         C15       0.98      1.00      0.99        42\n",
      "         C17       1.00      0.67      0.80         3\n",
      "         C18       0.00      0.00      0.00         0\n",
      "         C21       0.00      0.00      0.00         1\n",
      "         C31       0.00      0.00      0.00         0\n",
      "         E14       0.00      0.00      0.00         1\n",
      "         E21       0.93      0.72      0.81        18\n",
      "         E51       0.00      0.00      0.00         0\n",
      "         E71       1.00      0.71      0.83         7\n",
      "         M12       0.00      0.00      0.00         0\n",
      "         M14       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.86        72\n",
      "   macro avg       0.33      0.26      0.29        72\n",
      "weighted avg       0.94      0.86      0.89        72\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      C15\n",
       "1      C15\n",
       "2      C15\n",
       "3      C15\n",
       "4      E71\n",
       "      ... \n",
       "353    E21\n",
       "354    E71\n",
       "355    C15\n",
       "356    C15\n",
       "357    C15\n",
       "Name: labels, Length: 358, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "applyClassifier(enhancedDFList) # Comparing performance after using autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Neural Network using Enhanced Features\n",
      "Train on 434 samples, validate on 109 samples\n",
      "Epoch 1/10\n",
      "434/434 [==============================] - 6s 15ms/step - loss: 3.0160 - acc: 0.0369 - val_loss: 2.9762 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "434/434 [==============================] - 0s 146us/step - loss: 2.6514 - acc: 0.5346 - val_loss: 3.0554 - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "434/434 [==============================] - 0s 139us/step - loss: 2.2632 - acc: 0.5922 - val_loss: 3.2713 - val_acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "434/434 [==============================] - 0s 143us/step - loss: 1.8365 - acc: 0.5922 - val_loss: 4.0029 - val_acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "434/434 [==============================] - 0s 139us/step - loss: 1.8451 - acc: 0.5922 - val_loss: 4.4504 - val_acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "434/434 [==============================] - 0s 146us/step - loss: 1.8885 - acc: 0.5922 - val_loss: 4.1845 - val_acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "434/434 [==============================] - 0s 139us/step - loss: 1.7343 - acc: 0.5922 - val_loss: 3.7660 - val_acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "434/434 [==============================] - 0s 138us/step - loss: 1.6160 - acc: 0.5922 - val_loss: 3.4770 - val_acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "434/434 [==============================] - 0s 141us/step - loss: 1.5631 - acc: 0.5922 - val_loss: 3.3315 - val_acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "434/434 [==============================] - 0s 139us/step - loss: 1.6354 - acc: 0.5899 - val_loss: 3.2753 - val_acc: 0.0000e+00\n",
      "434/434 [==============================] - 0s 131us/step\n",
      "accuracy for Deep Neural Networks is\n",
      "0.5921658997162147\n",
      "Loss in Deep Neural Networks is\n",
      "1.7848782363575175\n"
     ]
    }
   ],
   "source": [
    "# Major Differences\n",
    "# Deep Neural Network using 3 layers is implemented and moreover the feature extraction is enhanced using auto encoder\n",
    "# Doc2Vec is used for vectorization of documents apart from assignment one as TF-IDF generates sparse matrix which is ineffic\n",
    "# Neural networks with 3 hidden layers are used to enhance the performance.\n",
    "def deepNeuralNet(rXtr, rXte, rYtr, rYte):\n",
    "    print(\"Deep Neural Network using Enhanced Features\")\n",
    "    le = LabelEncoder()\n",
    "    rYtr = le.fit_transform(rYtr)\n",
    "    rYte = le.fit_transform(rYte)\n",
    "    dup = numpy.unique(rYtr)\n",
    "    classifier = Sequential()\n",
    "    # First Hidden Layer\n",
    "    classifier.add(Dense(512, activation='relu', input_dim=rXtr.shape[1]))\n",
    "    classifier.add(Dropout(0.5))\n",
    "    # Second  Hidden Layer\n",
    "    classifier.add(Dense(512, activation='relu', input_dim=512))\n",
    "    classifier.add(Dropout(0.5))\n",
    "    classifier.add(Dense(512, activation='relu', input_dim=512))\n",
    "    classifier.add(Dropout(0.5))\n",
    "    classifier.add(Dense(512, activation='relu', input_dim=512))\n",
    "    classifier.add(Dropout(0.5))\n",
    "    # Output Layer\n",
    "    classifier.add(Dense(dup.size, activation='softmax'))\n",
    "    classifier.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    classifier.fit(rXtr, rYtr,validation_data=(rXte, rYte), batch_size=1024, epochs=10)\n",
    "    loss,accuracy = classifier.evaluate(rXtr,rYtr)\n",
    "    print(\"accuracy for Deep Neural Networks is\")\n",
    "    print(accuracy)\n",
    "    print(\"Loss in Deep Neural Networks is\")\n",
    "    print(loss)\n",
    "    \n",
    "def trainTestSplit(dataFrame):\n",
    "    targetF = dataFrame['labels']\n",
    "    splitDFF = dataFrame.iloc[:,:-1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(splitDFF, targetF, test_size=0.2, random_state=101)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    \n",
    "Xtr, Xte, Ytr, Yte = trainTestSplit(enhancedDFList[0])\n",
    "deepNeuralNet(Xtr, Xte, Ytr, Yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
